{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b4e15c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "83f26a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal #dùng cho môi trường liên tục\n",
    "from torch.distributions import Categorical #Lựa chọn hành động từ phân phối rời rạc.\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "09648e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d1bfd7",
   "metadata": {},
   "source": [
    "PAPER ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab618a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV_paper():\n",
    "    def __init__(self,\n",
    "                 lambda_rate,\n",
    "                 D_max,\n",
    "                 xi,\n",
    "                 max_power,\n",
    "                 snr_feedback,\n",
    "                 harq_type):\n",
    "        \n",
    "\n",
    "        # Tham số hệ thống\n",
    "        self.lambda_rate = lambda_rate  # Tốc độ đến trung bình (bit/slot)\n",
    "        self.D_max = D_max  # Độ trễ tối đa (slot)\n",
    "        self.xi = xi  # Xác suất vi phạm độ trễ mục tiêu\n",
    "        self.max_power = max_power  # Công suất tối đa\n",
    "        self.snr_feedback = snr_feedback  # Có phản hồi SNR hay không\n",
    "        self.harq_type = harq_type  # Loại HARQ: 'CC' hoặc 'IR'\n",
    "        self.n = 200  # Số lần sử dụng kênh mỗi slot\n",
    "        self.T = int(10 / xi)  # Số slot mỗi episode\n",
    "        self.Delta = 20 * max_power  # Hằng số phạt lớn\n",
    "        self.beta = 16  # Số mũ cho hàm phạt\n",
    "\n",
    "\n",
    "        # Không gian trạng thái\n",
    "        state_dim = 4 if not snr_feedback else 5\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=np.inf, shape=(state_dim,), dtype=np.float32)\n",
    "\n",
    "\n",
    "        # Không gian hành động: [R(t), p(t)]\n",
    "        self.action_space = gym.spaces.Box(low=np.array([0, 0]), high=np.array([np.inf, max_power]), dtype=np.float32)\n",
    "\n",
    "        # Bộ nhớ lịch sử A(t) cho D_max slot gần nhất\n",
    "        self.arrival_history = []\n",
    "        self.previous_snrs = []\n",
    "\n",
    "        # Khởi tạo trạng thái\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        #Đặt lại môi trường về trạng thái ban đầu\n",
    "        self.q_t = 0  # Độ dài hàng đợi\n",
    "        self.A_t = np.random.poisson(self.lambda_rate)  # Số bit đến\n",
    "        self.d_t = 0  # Số lần vi phạm độ trễ trong episode\n",
    "        self.k = 0  # Số lần truyền của gói tin hiện tại\n",
    "        self.t = 0  # đếm bước hiện tại\n",
    "        self.gamma_k = 0 if self.snr_feedback else None  # SNR còn lại\n",
    "        self.arrival_history = [self.A_t]\n",
    "        self.previous_snrs = []\n",
    "        state = [self.q_t, self.A_t, self.d_t, self.k]\n",
    "        if self.snr_feedback:\n",
    "            state.append(self.gamma_k)\n",
    "        return np.array(state)\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        R_t, p_t = action  # Tốc độ mã hóa và công suất truyền\n",
    "        self.t += 1\n",
    "\n",
    "\n",
    "        R_t = max(R_t, 0)\n",
    "        p_t = np.clip(p_t, 0, self.max_power)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Mô phỏng kênh fading Rayleigh\n",
    "        h = np.random.normal(0, 1) + 1j * np.random.normal(0, 1)\n",
    "        snr = p_t * (np.abs(h)**2)\n",
    "\n",
    "\n",
    "\n",
    "        # Xác định thành công truyền\n",
    "        if self.k == 0:  # Truyền mới\n",
    "            success = np.log2(1 + snr) >= R_t\n",
    "        else:  # Truyền lại\n",
    "            if self.harq_type == 'CC':\n",
    "                if self.snr_feedback:\n",
    "                    accumulated_snr = sum(self.previous_snrs) + snr\n",
    "                    success = np.log2(1 + accumulated_snr) >= R_t\n",
    "                else:\n",
    "                    # Giả định công suất không đổi qua các lần truyền lại\n",
    "                    # Cần xem lại vì mỗi lần truyền lại p_t không đổi nhưng có h khác nhau\n",
    "                    accumulated_snr = sum(self.previous_snrs) + snr\n",
    "                    success = np.log2(1 + accumulated_snr) >= R_t\n",
    "            elif self.harq_type == 'IR':\n",
    "                if self.snr_feedback:\n",
    "                    total_rate = sum([np.log2(1 + s) for s in self.previous_snrs]) + np.log2(1 + snr)\n",
    "                    success = total_rate >= R_t\n",
    "        \n",
    "\n",
    "        \n",
    "        # Tính toán tốc độ phục vụ S(t)\n",
    "        S_t = self.n * R_t if success else 0\n",
    "\n",
    "        # Cập nhật độ dài hàng đợi tạm thời\n",
    "        q_tmp = max(self.q_t + self.A_t - S_t, 0)\n",
    "\n",
    "\n",
    "        \n",
    "        # Tính q_th(t) dựa trên lịch sử A(t) trong D_max slot\n",
    "        if len(self.arrival_history) >= self.D_max:\n",
    "            q_th = sum(self.arrival_history[-self.D_max:])\n",
    "        else:\n",
    "            q_th = sum(self.arrival_history)\n",
    "        \n",
    "        # Kiểm tra vi phạm độ trễ\n",
    "        if q_tmp > q_th:\n",
    "            self.d_t += 1\n",
    "            w_t = self._calculate_penalty()\n",
    "            reward = -p_t - w_t\n",
    "        else:\n",
    "            reward = -p_t\n",
    "        \n",
    "        # Cập nhật hàng đợi với PODD\n",
    "        self.q_t = min(q_tmp, q_th)\n",
    "\n",
    "\n",
    "\n",
    "        # Cập nhật trạng thái\n",
    "        self.A_t = np.random.poisson(self.lambda_rate)\n",
    "        self.arrival_history.append(self.A_t)\n",
    "        if len(self.arrival_history) > self.D_max:\n",
    "            self.arrival_history.pop(0)\n",
    "        \n",
    "        if success:\n",
    "            self.k = 0\n",
    "            self.gamma_k = 0 if self.snr_feedback else None\n",
    "            self.previous_snrs = []\n",
    "        else:\n",
    "            self.k += 1\n",
    "            self.previous_snrs.append(snr)\n",
    "            if self.snr_feedback:\n",
    "                if self.harq_type == 'CC':\n",
    "                    self.gamma_k = max(2**R_t - 1 - sum(self.previous_snrs), 0)\n",
    "                elif self.harq_type == 'IR':\n",
    "                    self.gamma_k = max((2**R_t) / np.prod([1 + s for s in self.previous_snrs]) - 1, 0)\n",
    "        \n",
    "        state = [self.q_t, self.A_t, self.d_t, self.k]\n",
    "        if self.snr_feedback:\n",
    "            state.append(self.gamma_k)\n",
    "        \n",
    "        # Kiểm tra kết thúc episode (giả định đơn giản)\n",
    "        done = self.t >= self.T\n",
    "        \n",
    "        info = {'power': p_t, 'delay_violation': self.d_t, 'Rate': R_t}\n",
    "        \n",
    "        return np.array(state), reward, done, info\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _calculate_penalty(self):\n",
    "        #Tính toán giá trị phạt w(t) dựa trên số lần vi phạm độ trễ.\n",
    "        if self.d_t <= self.T * self.xi:\n",
    "            return self.Delta * (self.d_t / (self.T * self.xi)) ** self.beta\n",
    "        return self.Delta\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337bec74",
   "metadata": {},
   "source": [
    "Tạo RolloutBuffer, lưu data lại để train, tạo hàm để update lúc train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d51be318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RolloutBuffer Bộ nhớ tạm lưu giữ thông tin huấn luyện sau\n",
    "#lưu trữ tạm thời các quỹ đạo \n",
    "#tập hợp dữ liệu về các tương tác của tác nhân (agent) với môi trường trong một tập hợp (episode), tính hàm lợi thế\n",
    "\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = [] #Lưu các hành động đã thực hiện\n",
    "        self.states = []  #Lưu các trạng thái hiện tại\n",
    "        self.logprobs = []#Lưu log của xác suất pi(a|s)\n",
    "        self.rewards = [] #Lưu lại reward\n",
    "        self.state_values = [] # Giá trị trạng thái V do critic dự đoán, để tính hàm lợi thế\n",
    "        self.is_terminals = [] #Lưu lại cờ kết thúc\n",
    "\n",
    "\n",
    "    def clear(self): #xóa buffer để update data mới\n",
    "        del self.actions[:] \n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "#Actor: Mạng chính sách, xuất ra phân phối xác suất dựa trên trạng thái\n",
    "#Critic: Mạng giá trị, Ước lượng giá trị trạng thái (Vt) để tính hàm ưu tiên\n",
    "#hàm này giúp Agent chọn lọc hành động, đánh giá được trạng thái.\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std = None):\n",
    "        #state_dim: Số chiều của state\n",
    "        #action_dim: số chiều của action\n",
    "        #xem action có liên tục không\n",
    "        #action_std_init: độ lệch chuẩn cho phân phối liên tục\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        #Nếu hành động liên tục lưu lại chiều hành động chuyển đến phương sai action_std_init^2\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full((action_dim,), action_std * action_std).to(device)\n",
    "\n",
    "        # ACTOR\n",
    "        if has_continuous_action_space : # continuous action space\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            #nn.Tanh()\n",
    "        #Nếu là hành động liên tục thì đầu ra là trung bình u(s) phân phối Gauss => phân phối hành động π(a|s) = N(u,tổng)\n",
    "        #Tanh giới hạn đầu ra từ [-1,1], phù hợp vs phạm v hành động\n",
    "                        )\n",
    "        else: # discrete action space\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Softmax(dim=-1)\n",
    "                        )\n",
    "        #Không gian rời rạc: xuất ra xác suất cho từng action rời rạc theo Softmax\n",
    "\n",
    "        # CRITIC\n",
    "        # Tương tự như actor nhưng xuất ra là 1 giá trị đơn ước lượng giá trị trạng thái Vt\n",
    "        # Dùng để tính hàm lợi thế At = Q(s,a) - Vt\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 1)\n",
    "                    )\n",
    "        \n",
    "    #Cập nhật phương sai cho hành động liên tục\n",
    "    #tức là điều chỉnh khám phá, độ biến động cho các action\n",
    "    def set_action_std(self, new_action_std):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    \n",
    "    def act(self, state): #đầu vào là state hiện tại\n",
    "\n",
    "        # ACTOR\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state) #Nếu đầu vào là state liên tục thì đầu ra actor là giá trị trung bình u phân phối gauss\n",
    "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0) #phương sai Cov\n",
    "            dist = MultivariateNormal(action_mean, cov_mat) #tạo phân phối đa biến\n",
    "        else:\n",
    "            action_probs = self.actor(state) #đầu ra softmax\n",
    "            dist = Categorical(action_probs) #phân phối rời rạc\n",
    "\n",
    "        action = dist.sample() #chọn hành động bằng cách lấy mẫu phân phối \n",
    "        action_logprob = dist.log_prob(action) #lấy log của các suất được chọn (π(a|s))\n",
    "\n",
    "        # CRITIC\n",
    "        state_val = self.critic(state) #xuất ra giá Vt từ critic\n",
    "\n",
    "        return action.detach(), action_logprob.detach(), state_val.detach() #trả về action, log, Vt để lưu vào RolloutBuffer\n",
    "\n",
    "    #Dùng để tính toán lại các giá trị cần thiết khi cập nhật danh sách:\n",
    "        #+Log xác suất hành động \n",
    "        #+Entropy của chính sách\n",
    "        #+Ước lượng giá trị trạng thái Vt\n",
    "    # Đầu vào là state, action cũ đầu ra là logprob (mới), value (mới), entropy\n",
    "    # So sánh policy cũ và mới.\n",
    "    def evaluate(self, state, action):\n",
    "\n",
    "        # ACTOR\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            action_var = self.action_var.expand_as(action_mean)\n",
    "            cov_mat = torch.diag_embed(action_var).to(device)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "\n",
    "            # for single action continuous environments\n",
    "            if self.action_dim == 1:\n",
    "                action = action.reshape(-1, self.action_dim)\n",
    "\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "\n",
    "        # CRITIC\n",
    "        state_values = self.critic(state)\n",
    "\n",
    "        return action_logprobs, state_values, dist_entropy \n",
    "# Tại sao có act rồi cần dùng evaluate:\n",
    "#    + act dùng để action và thu thập dữ liệu xong lưu vào RolloutBuffer để train\n",
    "#    + evaluate dùng để tính toán train, policy cũ và mới: ratio, advantage, entropy.\n",
    "# act → buffer → evaluate → loss → update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "30c85aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, lambda_gae, K_epochs, eps_clip, has_continuous_action_space, action_std, minibatch_size=200):\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.lambda_gae = lambda_gae\n",
    "        self.minibatch_size = minibatch_size  # Thêm minibatch_size\n",
    "\n",
    "        self.buffer = RolloutBuffer()\n",
    "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "            {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "        ])\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if (self.action_std <= min_action_std):\n",
    "                self.action_std = min_action_std\n",
    "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
    "            else:\n",
    "                print(\"setting actor output action_std to : \", self.action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "        else:\n",
    "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if self.has_continuous_action_space:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "            return action.detach().cpu().numpy().flatten()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "            return action.item()\n",
    "\n",
    "    def update(self):\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
    "        is_terminals = torch.tensor(self.buffer.is_terminals, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Compute advantages using GAE\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        gae = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_value = 0  # No next state value at the end\n",
    "            else:\n",
    "                next_value = old_state_values[t + 1] * (1 - is_terminals[t + 1])\n",
    "            delta = rewards[t] + self.gamma * next_value - old_state_values[t]\n",
    "            gae = delta + self.gamma * self.lambda_gae * (1 - is_terminals[t]) * gae\n",
    "            advantages[t] = gae\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-7)\n",
    "\n",
    "        # Create TensorDataset and DataLoader\n",
    "        dataset = TensorDataset(old_states, old_actions, old_logprobs, rewards, advantages)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.minibatch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "            for batch in dataloader:\n",
    "                batch_states, batch_actions, batch_logprobs, batch_rewards, batch_advantages = batch\n",
    "                logprobs, state_values, dist_entropy = self.policy.evaluate(batch_states, batch_actions)\n",
    "                state_values = torch.squeeze(state_values)\n",
    "                ratios = torch.exp(logprobs - batch_logprobs.detach())\n",
    "                surr1 = ratios * batch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
    "                loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, batch_rewards) - 0.01 * dist_entropy\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.mean().backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # Clear buffer\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0f94ce84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------\n",
      "max training timesteps :  2000000\n",
      "max timesteps per episode :  1000\n",
      "log frequency : 2000 timesteps\n",
      "printing average reward over episodes in last : 4000 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "state space dimension :  5\n",
      "action space dimension :  2\n",
      "--------------------------------------------------------------------------------------------\n",
      "Initializing a continuous action space policy\n",
      "--------------------------------------------------------------------------------------------\n",
      "starting std of action distribution :  0.5\n",
      "decay rate of std of action distribution :  0.005\n",
      "minimum std of action distribution :  0.1\n",
      "decay frequency of std of action distribution : 20000 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "PPO update frequency : 2048 timesteps\n",
      "PPO K epochs :  80\n",
      "PPO epsilon clip :  0.2\n",
      "discount factor (gamma) :  0.99\n",
      "--------------------------------------------------------------------------------------------\n",
      "optimizer learning rate actor :  0.0002\n",
      "optimizer learning rate critic :  0.0002\n",
      "============================================================================================\n",
      "Episode  0 : Reward =  -370345.25\n",
      "Saving better model at episode 0 with reward -370345.25\n",
      "Episode  1 : Reward =  -371473.4\n",
      "Episode  2 : Reward =  -372607.7\n",
      "Episode  3 : Reward =  -369939.06\n",
      "Saving better model at episode 3 with reward -369939.0625\n",
      "Episode  4 : Reward =  -369938.34\n",
      "Saving better model at episode 4 with reward -369938.34375\n",
      "Episode  5 : Reward =  -371451.56\n",
      "Episode  6 : Reward =  -369932.2\n",
      "Saving better model at episode 6 with reward -369932.1875\n",
      "Episode  7 : Reward =  -370686.72\n",
      "Episode  8 : Reward =  -371810.3\n",
      "Episode  9 : Reward =  -368419.78\n",
      "Saving better model at episode 9 with reward -368419.78125\n",
      "Episode  10 : Reward =  -368784.38\n",
      "Episode  11 : Reward =  -371053.9\n",
      "Episode  12 : Reward =  -368034.62\n",
      "Saving better model at episode 12 with reward -368034.625\n",
      "Episode  13 : Reward =  -369552.62\n",
      "Episode  14 : Reward =  -371447.62\n",
      "Episode  15 : Reward =  -371451.06\n",
      "Episode  16 : Reward =  -370694.47\n",
      "Episode  17 : Reward =  -372192.34\n",
      "Episode  18 : Reward =  -370313.0\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.495\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  19 : Reward =  -371441.88\n",
      "Episode  20 : Reward =  -371459.75\n",
      "Episode  21 : Reward =  -369941.44\n",
      "Episode  22 : Reward =  -369939.03\n",
      "Episode  23 : Reward =  -369932.06\n",
      "Episode  24 : Reward =  -369194.22\n",
      "Episode  25 : Reward =  -369948.7\n",
      "Episode  26 : Reward =  -371077.97\n",
      "Episode  27 : Reward =  -369569.47\n",
      "Episode  28 : Reward =  -369954.94\n",
      "Episode  29 : Reward =  -367694.56\n",
      "Saving better model at episode 29 with reward -367694.5625\n",
      "Episode  30 : Reward =  -368839.4\n",
      "Episode  31 : Reward =  -368824.94\n",
      "Episode  32 : Reward =  -371093.53\n",
      "Episode  33 : Reward =  -371110.88\n",
      "Episode  34 : Reward =  -368830.66\n",
      "Episode  35 : Reward =  -371832.72\n",
      "Episode  36 : Reward =  -368446.06\n",
      "Episode  37 : Reward =  -369951.1\n",
      "Episode  38 : Reward =  -369185.0\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.49\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  39 : Reward =  -369953.5\n",
      "Episode  40 : Reward =  -370315.44\n",
      "Episode  41 : Reward =  -370312.03\n",
      "Episode  42 : Reward =  -370695.66\n",
      "Episode  43 : Reward =  -370330.62\n",
      "Episode  44 : Reward =  -370320.56\n",
      "Episode  45 : Reward =  -371097.53\n",
      "Episode  46 : Reward =  -370339.47\n",
      "Episode  47 : Reward =  -367341.03\n",
      "Saving better model at episode 47 with reward -367341.03125\n",
      "Episode  48 : Reward =  -369971.7\n",
      "Episode  49 : Reward =  -370359.47\n",
      "Episode  50 : Reward =  -367325.66\n",
      "Saving better model at episode 50 with reward -367325.65625\n",
      "Episode  51 : Reward =  -366952.6\n",
      "Saving better model at episode 51 with reward -366952.59375\n",
      "Episode  52 : Reward =  -368471.03\n",
      "Episode  53 : Reward =  -368441.16\n",
      "Episode  54 : Reward =  -366165.16\n",
      "Saving better model at episode 54 with reward -366165.15625\n",
      "Episode  55 : Reward =  -369190.78\n",
      "Episode  56 : Reward =  -365782.53\n",
      "Saving better model at episode 56 with reward -365782.53125\n",
      "Episode  57 : Reward =  -369203.6\n",
      "Episode  58 : Reward =  -368051.88\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.485\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  59 : Reward =  -369560.88\n",
      "Episode  60 : Reward =  -367290.62\n",
      "Episode  61 : Reward =  -369945.97\n",
      "Episode  62 : Reward =  -369546.75\n",
      "Episode  63 : Reward =  -366553.8\n",
      "Episode  64 : Reward =  -366932.1\n",
      "Episode  65 : Reward =  -366916.38\n",
      "Episode  66 : Reward =  -369195.3\n",
      "Episode  67 : Reward =  -368443.94\n",
      "Episode  68 : Reward =  -367291.97\n",
      "Episode  69 : Reward =  -363141.03\n",
      "Saving better model at episode 69 with reward -363141.03125\n",
      "Episode  70 : Reward =  -365795.38\n",
      "Episode  71 : Reward =  -367685.1\n",
      "Episode  72 : Reward =  -369573.72\n",
      "Episode  73 : Reward =  -363919.0\n",
      "Episode  74 : Reward =  -366923.7\n",
      "Episode  75 : Reward =  -367304.5\n",
      "Episode  76 : Reward =  -364293.44\n",
      "Episode  77 : Reward =  -368456.2\n",
      "Episode  78 : Reward =  -362788.22\n",
      "Saving better model at episode 78 with reward -362788.21875\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.48\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  79 : Reward =  -366952.22\n",
      "Episode  80 : Reward =  -360905.78\n",
      "Saving better model at episode 80 with reward -360905.78125\n",
      "Episode  81 : Reward =  -363562.47\n",
      "Episode  82 : Reward =  -367352.5\n",
      "Episode  83 : Reward =  -365458.3\n",
      "Episode  84 : Reward =  -362415.44\n",
      "Episode  85 : Reward =  -359389.2\n",
      "Saving better model at episode 85 with reward -359389.1875\n",
      "Episode  86 : Reward =  -359414.75\n",
      "Episode  87 : Reward =  -365079.34\n",
      "Episode  88 : Reward =  -362449.22\n",
      "Episode  89 : Reward =  -369631.84\n",
      "Episode  90 : Reward =  -365851.72\n",
      "Episode  91 : Reward =  -364321.44\n",
      "Episode  92 : Reward =  -360170.8\n",
      "Episode  93 : Reward =  -358674.38\n",
      "Saving better model at episode 93 with reward -358674.375\n",
      "Episode  94 : Reward =  -360167.7\n",
      "Episode  95 : Reward =  -360547.12\n",
      "Episode  96 : Reward =  -360177.06\n",
      "Episode  97 : Reward =  -360566.53\n",
      "Episode  98 : Reward =  -354142.62\n",
      "Saving better model at episode 98 with reward -354142.625\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.475\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  99 : Reward =  -356040.94\n",
      "Episode  100 : Reward =  -358315.22\n",
      "Episode  101 : Reward =  -359066.88\n",
      "Episode  102 : Reward =  -360968.56\n",
      "Episode  103 : Reward =  -361319.47\n",
      "Episode  104 : Reward =  -361713.3\n",
      "Episode  105 : Reward =  -356053.38\n",
      "Episode  106 : Reward =  -355299.88\n",
      "Episode  107 : Reward =  -354541.25\n",
      "Episode  108 : Reward =  -355284.9\n",
      "Episode  109 : Reward =  -355314.25\n",
      "Episode  110 : Reward =  -353809.16\n",
      "Saving better model at episode 110 with reward -353809.15625\n",
      "Episode  111 : Reward =  -360227.53\n",
      "Episode  112 : Reward =  -354935.1\n",
      "Episode  113 : Reward =  -356819.94\n",
      "Episode  114 : Reward =  -355310.0\n",
      "Episode  115 : Reward =  -360201.44\n",
      "Episode  116 : Reward =  -358712.66\n",
      "Episode  117 : Reward =  -353413.7\n",
      "Saving better model at episode 117 with reward -353413.6875\n",
      "Episode  118 : Reward =  -356075.53\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.47\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  119 : Reward =  -348514.7\n",
      "Saving better model at episode 119 with reward -348514.6875\n",
      "Episode  120 : Reward =  -353061.72\n",
      "Episode  121 : Reward =  -356472.62\n",
      "Episode  122 : Reward =  -352674.3\n",
      "Episode  123 : Reward =  -353062.75\n",
      "Episode  124 : Reward =  -351153.97\n",
      "Episode  125 : Reward =  -357955.47\n",
      "Episode  126 : Reward =  -353432.75\n",
      "Episode  127 : Reward =  -354583.34\n",
      "Episode  128 : Reward =  -354577.16\n",
      "Episode  129 : Reward =  -353458.1\n",
      "Episode  130 : Reward =  -351182.6\n",
      "Episode  131 : Reward =  -354211.1\n",
      "Episode  132 : Reward =  -361741.34\n",
      "Episode  133 : Reward =  -359877.5\n",
      "Episode  134 : Reward =  -355711.8\n",
      "Episode  135 : Reward =  -351584.28\n",
      "Episode  136 : Reward =  -356495.03\n",
      "Episode  137 : Reward =  -352354.34\n",
      "Episode  138 : Reward =  -361027.2\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.465\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  139 : Reward =  -353478.0\n",
      "Episode  140 : Reward =  -355002.66\n",
      "Episode  141 : Reward =  -350049.72\n",
      "Episode  142 : Reward =  -352723.88\n",
      "Episode  143 : Reward =  -346307.3\n",
      "Saving better model at episode 143 with reward -346307.3125\n",
      "Episode  144 : Reward =  -354251.62\n",
      "Episode  145 : Reward =  -361058.12\n",
      "Episode  146 : Reward =  -356886.7\n",
      "Episode  147 : Reward =  -339902.12\n",
      "Saving better model at episode 147 with reward -339902.125\n",
      "Episode  148 : Reward =  -342570.66\n",
      "Episode  149 : Reward =  -343679.62\n",
      "Episode  150 : Reward =  -345196.6\n",
      "Episode  151 : Reward =  -338417.8\n",
      "Saving better model at episode 151 with reward -338417.8125\n",
      "Episode  152 : Reward =  -340311.34\n",
      "Episode  153 : Reward =  -352010.66\n",
      "Episode  154 : Reward =  -334646.16\n",
      "Saving better model at episode 154 with reward -334646.15625\n",
      "Episode  155 : Reward =  -341062.75\n",
      "Episode  156 : Reward =  -331255.6\n",
      "Saving better model at episode 156 with reward -331255.59375\n",
      "Episode  157 : Reward =  -320303.47\n",
      "Saving better model at episode 157 with reward -320303.46875\n",
      "Episode  158 : Reward =  -346338.7\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.46\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  159 : Reward =  -347467.22\n",
      "Episode  160 : Reward =  -350499.06\n",
      "Episode  161 : Reward =  -346349.1\n",
      "Episode  162 : Reward =  -356117.3\n",
      "Episode  163 : Reward =  -330096.75\n",
      "Episode  164 : Reward =  -340692.16\n",
      "Episode  165 : Reward =  -338840.7\n",
      "Episode  166 : Reward =  -341081.0\n",
      "Episode  167 : Reward =  -352058.4\n",
      "Episode  168 : Reward =  -321477.8\n",
      "Episode  169 : Reward =  -344891.75\n",
      "Episode  170 : Reward =  -348684.72\n",
      "Episode  171 : Reward =  -348660.9\n",
      "Episode  172 : Reward =  -341457.44\n",
      "Episode  173 : Reward =  -337720.16\n",
      "Episode  174 : Reward =  -344490.62\n",
      "Episode  175 : Reward =  -343370.94\n",
      "Episode  176 : Reward =  -355080.47\n",
      "Episode  177 : Reward =  -350516.2\n",
      "Episode  178 : Reward =  -345231.47\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.455\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  179 : Reward =  -352408.53\n",
      "Episode  180 : Reward =  -335013.66\n",
      "Episode  181 : Reward =  -336912.4\n",
      "Episode  182 : Reward =  -282517.7\n",
      "Saving better model at episode 182 with reward -282517.6875\n",
      "Episode  183 : Reward =  -310839.2\n",
      "Episode  184 : Reward =  -338027.12\n",
      "Episode  185 : Reward =  -346358.72\n",
      "Episode  186 : Reward =  -345185.44\n",
      "Episode  187 : Reward =  -336842.1\n",
      "Episode  188 : Reward =  -342524.7\n",
      "Episode  189 : Reward =  -326695.6\n",
      "Episode  190 : Reward =  -333842.53\n",
      "Episode  191 : Reward =  -325171.3\n",
      "Episode  192 : Reward =  -310449.3\n",
      "Episode  193 : Reward =  -293117.75\n",
      "Episode  194 : Reward =  -316124.22\n",
      "Episode  195 : Reward =  -331221.12\n",
      "Episode  196 : Reward =  -336477.44\n",
      "Episode  197 : Reward =  -341772.0\n",
      "Episode  198 : Reward =  -295325.03\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.45\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  199 : Reward =  -251518.61\n",
      "Saving better model at episode 199 with reward -251518.609375\n",
      "Episode  200 : Reward =  -299472.4\n",
      "Episode  201 : Reward =  -316088.25\n",
      "Episode  202 : Reward =  -340242.97\n",
      "Episode  203 : Reward =  -324347.7\n",
      "Episode  204 : Reward =  -335349.94\n",
      "Episode  205 : Reward =  -329669.8\n",
      "Episode  206 : Reward =  -317217.66\n",
      "Episode  207 : Reward =  -302494.97\n",
      "Episode  208 : Reward =  -308146.34\n",
      "Episode  209 : Reward =  -323297.34\n",
      "Episode  210 : Reward =  -306303.94\n",
      "Episode  211 : Reward =  -318024.12\n",
      "Episode  212 : Reward =  -299500.4\n",
      "Episode  213 : Reward =  -250391.03\n",
      "Saving better model at episode 213 with reward -250391.03125\n",
      "Episode  214 : Reward =  -257225.03\n",
      "Episode  215 : Reward =  -280983.16\n",
      "Episode  216 : Reward =  -331605.84\n",
      "Episode  217 : Reward =  -302128.44\n",
      "Episode  218 : Reward =  -314595.4\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.445\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  219 : Reward =  -194179.6\n",
      "Saving better model at episode 219 with reward -194179.59375\n",
      "Episode  220 : Reward =  -293436.06\n",
      "Episode  221 : Reward =  -295711.3\n",
      "Episode  222 : Reward =  -173746.22\n",
      "Saving better model at episode 222 with reward -173746.21875\n",
      "Episode  223 : Reward =  -242096.05\n",
      "Episode  224 : Reward =  -226240.45\n",
      "Episode  225 : Reward =  -195361.6\n",
      "Episode  226 : Reward =  -259096.06\n",
      "Episode  227 : Reward =  -254940.03\n",
      "Episode  228 : Reward =  -277235.25\n",
      "Episode  229 : Reward =  -256463.23\n",
      "Episode  230 : Reward =  -309722.12\n",
      "Episode  231 : Reward =  -270455.28\n",
      "Episode  232 : Reward =  -319544.53\n",
      "Episode  233 : Reward =  -222871.9\n",
      "Episode  234 : Reward =  -218364.42\n",
      "Episode  235 : Reward =  -305598.84\n",
      "Episode  236 : Reward =  -261772.88\n",
      "Episode  237 : Reward =  -250069.47\n",
      "Episode  238 : Reward =  -176133.84\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.44\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  239 : Reward =  -269681.12\n",
      "Episode  240 : Reward =  -19069.395\n",
      "Saving better model at episode 240 with reward -19069.39453125\n",
      "Episode  241 : Reward =  -107410.734\n",
      "Episode  242 : Reward =  -116479.8\n",
      "Episode  243 : Reward =  -245617.7\n",
      "Episode  244 : Reward =  -169753.92\n",
      "Episode  245 : Reward =  -272765.97\n",
      "Episode  246 : Reward =  -242931.4\n",
      "Episode  247 : Reward =  -142911.62\n",
      "Episode  248 : Reward =  -231625.06\n",
      "Episode  249 : Reward =  -204823.45\n",
      "Episode  250 : Reward =  -279152.44\n",
      "Episode  251 : Reward =  -309313.03\n",
      "Episode  252 : Reward =  -234577.31\n",
      "Episode  253 : Reward =  -272729.7\n",
      "Episode  254 : Reward =  -291573.8\n",
      "Episode  255 : Reward =  -308535.56\n",
      "Episode  256 : Reward =  -334978.44\n",
      "Episode  257 : Reward =  -181116.9\n",
      "Episode  258 : Reward =  -199959.17\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.435\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  259 : Reward =  -97643.28\n",
      "Episode  260 : Reward =  -173073.9\n",
      "Episode  261 : Reward =  -287826.53\n",
      "Episode  262 : Reward =  -174239.3\n",
      "Episode  263 : Reward =  -171548.88\n",
      "Episode  264 : Reward =  -205157.1\n",
      "Episode  265 : Reward =  -206685.58\n",
      "Episode  266 : Reward =  -251951.81\n",
      "Episode  267 : Reward =  -228974.16\n",
      "Episode  268 : Reward =  -165946.94\n",
      "Episode  269 : Reward =  -81725.65\n",
      "Episode  270 : Reward =  -275370.97\n",
      "Episode  271 : Reward =  -210504.05\n",
      "Episode  272 : Reward =  -189337.67\n",
      "Episode  273 : Reward =  -278767.25\n",
      "Episode  274 : Reward =  -296485.44\n",
      "Episode  275 : Reward =  -296851.25\n",
      "Episode  276 : Reward =  -319899.53\n",
      "Episode  277 : Reward =  -286333.1\n",
      "Episode  278 : Reward =  -253920.6\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.43\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  279 : Reward =  -209053.58\n",
      "Episode  280 : Reward =  -254299.19\n",
      "Episode  281 : Reward =  -283721.25\n",
      "Episode  282 : Reward =  -152101.92\n",
      "Episode  283 : Reward =  -232056.55\n",
      "Episode  284 : Reward =  -263344.97\n",
      "Episode  285 : Reward =  -311963.47\n",
      "Episode  286 : Reward =  -245636.61\n",
      "Episode  287 : Reward =  -19896.12\n",
      "Episode  288 : Reward =  -304819.3\n",
      "Episode  289 : Reward =  -218496.44\n",
      "Episode  290 : Reward =  -321460.28\n",
      "Episode  291 : Reward =  -279569.84\n",
      "Episode  292 : Reward =  -309720.53\n",
      "Episode  293 : Reward =  -296590.5\n",
      "Episode  294 : Reward =  -247142.45\n",
      "Episode  295 : Reward =  -237341.33\n",
      "Episode  296 : Reward =  -153232.5\n",
      "Episode  297 : Reward =  -164561.05\n",
      "Episode  298 : Reward =  -177016.66\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.425\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  299 : Reward =  -120816.73\n",
      "Episode  300 : Reward =  -197085.97\n",
      "Episode  301 : Reward =  -130178.984\n",
      "Episode  302 : Reward =  -93978.695\n",
      "Episode  303 : Reward =  -16542.275\n",
      "Saving better model at episode 303 with reward -16542.275390625\n",
      "Episode  304 : Reward =  -74375.34\n",
      "Episode  305 : Reward =  -72511.87\n",
      "Episode  306 : Reward =  -83073.625\n",
      "Episode  307 : Reward =  -18926.326\n",
      "Episode  308 : Reward =  -44127.37\n",
      "Episode  309 : Reward =  -19974.477\n",
      "Episode  310 : Reward =  -17348.543\n",
      "Episode  311 : Reward =  -14703.829\n",
      "Saving better model at episode 311 with reward -14703.8291015625\n",
      "Episode  312 : Reward =  -31410.527\n",
      "Episode  313 : Reward =  -13198.562\n",
      "Saving better model at episode 313 with reward -13198.5615234375\n",
      "Episode  314 : Reward =  -14271.349\n",
      "Episode  315 : Reward =  -15404.694\n",
      "Episode  316 : Reward =  -11673.506\n",
      "Saving better model at episode 316 with reward -11673.505859375\n",
      "Episode  317 : Reward =  -16930.295\n",
      "Episode  318 : Reward =  -16572.014\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.42\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  319 : Reward =  -12782.366\n",
      "Episode  320 : Reward =  -18109.955\n",
      "Episode  321 : Reward =  -16703.158\n",
      "Episode  322 : Reward =  -14269.605\n",
      "Episode  323 : Reward =  -13247.832\n",
      "Episode  324 : Reward =  -9734.798\n",
      "Saving better model at episode 324 with reward -9734.7978515625\n",
      "Episode  325 : Reward =  -10124.606\n",
      "Episode  326 : Reward =  -10916.333\n",
      "Episode  327 : Reward =  -12392.632\n",
      "Episode  328 : Reward =  -10866.657\n",
      "Episode  329 : Reward =  -17386.707\n",
      "Episode  330 : Reward =  -10225.398\n",
      "Episode  331 : Reward =  -9439.089\n",
      "Saving better model at episode 331 with reward -9439.0888671875\n",
      "Episode  332 : Reward =  -11422.246\n",
      "Episode  333 : Reward =  -11279.709\n",
      "Episode  334 : Reward =  -8334.739\n",
      "Saving better model at episode 334 with reward -8334.7392578125\n",
      "Episode  335 : Reward =  -9861.531\n",
      "Episode  336 : Reward =  -9067.619\n",
      "Episode  337 : Reward =  -11377.651\n",
      "Episode  338 : Reward =  -9858.311\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.415\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  339 : Reward =  -8729.812\n",
      "Episode  340 : Reward =  -9465.912\n",
      "Episode  341 : Reward =  -12471.789\n",
      "Episode  342 : Reward =  -8765.634\n",
      "Episode  343 : Reward =  -9052.775\n",
      "Episode  344 : Reward =  -8042.2007\n",
      "Saving better model at episode 344 with reward -8042.20068359375\n",
      "Episode  345 : Reward =  -8329.134\n",
      "Episode  346 : Reward =  -9454.739\n",
      "Episode  347 : Reward =  -8297.275\n",
      "Episode  348 : Reward =  -9478.736\n",
      "Episode  349 : Reward =  -7937.575\n",
      "Saving better model at episode 349 with reward -7937.5751953125\n",
      "Episode  350 : Reward =  -7649.662\n",
      "Saving better model at episode 350 with reward -7649.662109375\n",
      "Episode  351 : Reward =  -8406.597\n",
      "Episode  352 : Reward =  -7290.652\n",
      "Saving better model at episode 352 with reward -7290.65185546875\n",
      "Episode  353 : Reward =  -6879.3354\n",
      "Saving better model at episode 353 with reward -6879.33544921875\n",
      "Episode  354 : Reward =  -8057.061\n",
      "Episode  355 : Reward =  -10277.33\n",
      "Episode  356 : Reward =  -7281.1494\n",
      "Episode  357 : Reward =  -9123.747\n",
      "Episode  358 : Reward =  -7297.9883\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.41\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  359 : Reward =  -7627.921\n",
      "Episode  360 : Reward =  -8011.097\n",
      "Episode  361 : Reward =  -7640.8525\n",
      "Episode  362 : Reward =  -9623.867\n",
      "Episode  363 : Reward =  -8861.545\n",
      "Episode  364 : Reward =  -9565.743\n",
      "Episode  365 : Reward =  -7654.917\n",
      "Episode  366 : Reward =  -6623.1177\n",
      "Saving better model at episode 366 with reward -6623.11767578125\n",
      "Episode  367 : Reward =  -8116.644\n",
      "Episode  368 : Reward =  -9582.708\n",
      "Episode  369 : Reward =  -8847.927\n",
      "Episode  370 : Reward =  -5473.0244\n",
      "Saving better model at episode 370 with reward -5473.0244140625\n",
      "Episode  371 : Reward =  -8384.531\n",
      "Episode  372 : Reward =  -8023.2695\n",
      "Episode  373 : Reward =  -6525.59\n",
      "Episode  374 : Reward =  -7336.379\n",
      "Episode  375 : Reward =  -8478.144\n",
      "Episode  376 : Reward =  -10285.991\n",
      "Episode  377 : Reward =  -9535.613\n",
      "Episode  378 : Reward =  -6930.9824\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.405\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  379 : Reward =  -9482.036\n",
      "Episode  380 : Reward =  -7265.084\n",
      "Episode  381 : Reward =  -6879.7417\n",
      "Episode  382 : Reward =  -9186.673\n",
      "Episode  383 : Reward =  -8786.218\n",
      "Episode  384 : Reward =  -5419.309\n",
      "Saving better model at episode 384 with reward -5419.30908203125\n",
      "Episode  385 : Reward =  -6907.1577\n",
      "Episode  386 : Reward =  -6120.2056\n",
      "Episode  387 : Reward =  -6823.3696\n",
      "Episode  388 : Reward =  -4256.339\n",
      "Saving better model at episode 388 with reward -4256.3388671875\n",
      "Episode  389 : Reward =  -6889.5273\n",
      "Episode  390 : Reward =  -5797.0713\n",
      "Episode  391 : Reward =  -5356.037\n",
      "Episode  392 : Reward =  -6945.0034\n",
      "Episode  393 : Reward =  -6573.649\n",
      "Episode  394 : Reward =  -6577.3076\n",
      "Episode  395 : Reward =  -5829.432\n",
      "Episode  396 : Reward =  -6968.8555\n",
      "Episode  397 : Reward =  -5362.6235\n",
      "Episode  398 : Reward =  -6144.2227\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.4\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  399 : Reward =  -5030.6284\n",
      "Episode  400 : Reward =  -5083.903\n",
      "Episode  401 : Reward =  -3512.5015\n",
      "Saving better model at episode 401 with reward -3512.50146484375\n",
      "Episode  402 : Reward =  -5033.556\n",
      "Episode  403 : Reward =  -5816.6904\n",
      "Episode  404 : Reward =  -6239.931\n",
      "Episode  405 : Reward =  -5880.5444\n",
      "Episode  406 : Reward =  -4001.9917\n",
      "Episode  407 : Reward =  -5550.673\n",
      "Episode  408 : Reward =  -7800.264\n",
      "Episode  409 : Reward =  -5447.178\n",
      "Episode  410 : Reward =  -6684.338\n",
      "Episode  411 : Reward =  -5829.979\n",
      "Episode  412 : Reward =  -7452.0146\n",
      "Episode  413 : Reward =  -4333.1255\n",
      "Episode  414 : Reward =  -6252.73\n",
      "Episode  415 : Reward =  -5125.2124\n",
      "Episode  416 : Reward =  -5096.2827\n",
      "Episode  417 : Reward =  -5909.854\n",
      "Episode  418 : Reward =  -5606.7314\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.395\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  419 : Reward =  -5483.148\n",
      "Episode  420 : Reward =  -4700.923\n",
      "Episode  421 : Reward =  -4331.8965\n",
      "Episode  422 : Reward =  -6278.1714\n",
      "Episode  423 : Reward =  -7042.813\n",
      "Episode  424 : Reward =  -5165.7534\n",
      "Episode  425 : Reward =  -5529.1724\n",
      "Episode  426 : Reward =  -5531.516\n",
      "Episode  427 : Reward =  -1624.4143\n",
      "Saving better model at episode 427 with reward -1624.414306640625\n",
      "Episode  428 : Reward =  -6276.5156\n",
      "Episode  429 : Reward =  -4696.8394\n",
      "Episode  430 : Reward =  -4354.9316\n",
      "Episode  431 : Reward =  -5511.5723\n",
      "Episode  432 : Reward =  -6262.1743\n",
      "Episode  433 : Reward =  -4350.469\n",
      "Episode  434 : Reward =  -6278.3955\n",
      "Episode  435 : Reward =  -4413.9272\n",
      "Episode  436 : Reward =  -3923.5176\n",
      "Episode  437 : Reward =  -8489.365\n",
      "Episode  438 : Reward =  -4718.31\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.39\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  439 : Reward =  -6611.8037\n",
      "Episode  440 : Reward =  -5490.824\n",
      "Episode  441 : Reward =  -5070.9404\n",
      "Episode  442 : Reward =  -4304.303\n",
      "Episode  443 : Reward =  -6575.0845\n",
      "Episode  444 : Reward =  -5099.6426\n",
      "Episode  445 : Reward =  -4365.065\n",
      "Episode  446 : Reward =  -4384.947\n",
      "Episode  447 : Reward =  -2394.4548\n",
      "Episode  448 : Reward =  -7387.206\n",
      "Episode  449 : Reward =  -6636.3286\n",
      "Episode  450 : Reward =  -6634.191\n",
      "Episode  451 : Reward =  -6233.8804\n",
      "Episode  452 : Reward =  -5876.0586\n",
      "Episode  453 : Reward =  -5153.413\n",
      "Episode  454 : Reward =  -6685.64\n",
      "Episode  455 : Reward =  -5534.2427\n",
      "Episode  456 : Reward =  -4811.8887\n",
      "Episode  457 : Reward =  -4412.754\n",
      "Episode  458 : Reward =  -4748.694\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.385\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  459 : Reward =  -3967.1235\n",
      "Episode  460 : Reward =  -5131.892\n",
      "Episode  461 : Reward =  -5530.1445\n",
      "Episode  462 : Reward =  -5908.8003\n",
      "Episode  463 : Reward =  -4715.2446\n",
      "Episode  464 : Reward =  -4701.6133\n",
      "Episode  465 : Reward =  -1631.9569\n",
      "Episode  466 : Reward =  -4742.3755\n",
      "Episode  467 : Reward =  -5484.016\n",
      "Episode  468 : Reward =  -3205.9387\n",
      "Episode  469 : Reward =  -3948.7722\n",
      "Episode  470 : Reward =  -3225.3809\n",
      "Episode  471 : Reward =  -3673.6401\n",
      "Episode  472 : Reward =  -5560.9204\n",
      "Episode  473 : Reward =  -3600.5007\n",
      "Episode  474 : Reward =  -4370.559\n",
      "Episode  475 : Reward =  -3596.8904\n",
      "Episode  476 : Reward =  -3922.2585\n",
      "Episode  477 : Reward =  -3167.8936\n",
      "Episode  478 : Reward =  -2726.5957\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.38\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  479 : Reward =  -1904.0793\n",
      "Episode  480 : Reward =  -5446.3745\n",
      "Episode  481 : Reward =  -2780.3618\n",
      "Episode  482 : Reward =  -2026.4581\n",
      "Episode  483 : Reward =  -4333.7607\n",
      "Episode  484 : Reward =  -3559.9946\n",
      "Episode  485 : Reward =  -1995.4929\n",
      "Episode  486 : Reward =  -4718.579\n",
      "Episode  487 : Reward =  -5156.142\n",
      "Episode  488 : Reward =  -5109.076\n",
      "Episode  489 : Reward =  -2710.9536\n",
      "Episode  490 : Reward =  -3974.4065\n",
      "Episode  491 : Reward =  -3108.8289\n",
      "Episode  492 : Reward =  -5538.6143\n",
      "Episode  493 : Reward =  -3120.4763\n",
      "Episode  494 : Reward =  -6306.112\n",
      "Episode  495 : Reward =  -4729.6377\n",
      "Episode  496 : Reward =  -2874.417\n",
      "Episode  497 : Reward =  -1933.6847\n",
      "Episode  498 : Reward =  -3133.7463\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.375\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  499 : Reward =  -2361.4265\n",
      "Episode  500 : Reward =  -3517.2502\n",
      "Episode  501 : Reward =  -3585.9053\n",
      "Episode  502 : Reward =  -2063.5518\n",
      "Episode  503 : Reward =  -2432.3508\n",
      "Episode  504 : Reward =  -4690.9414\n",
      "Episode  505 : Reward =  -4735.592\n",
      "Episode  506 : Reward =  -3601.0227\n",
      "Episode  507 : Reward =  -2411.5845\n",
      "Episode  508 : Reward =  -3207.8577\n",
      "Episode  509 : Reward =  -5500.969\n",
      "Episode  510 : Reward =  -2814.7107\n",
      "Episode  511 : Reward =  -2825.4246\n",
      "Episode  512 : Reward =  -1672.4252\n",
      "Episode  513 : Reward =  -3170.3384\n",
      "Episode  514 : Reward =  -6306.9287\n",
      "Episode  515 : Reward =  -3216.6152\n",
      "Episode  516 : Reward =  -2832.8203\n",
      "Episode  517 : Reward =  -3295.7383\n",
      "Episode  518 : Reward =  -4003.9497\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.37\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  519 : Reward =  -3241.8435\n",
      "Episode  520 : Reward =  -4068.3503\n",
      "Episode  521 : Reward =  -2485.596\n",
      "Episode  522 : Reward =  -1676.4357\n",
      "Episode  523 : Reward =  -2520.5605\n",
      "Episode  524 : Reward =  -2465.2073\n",
      "Episode  525 : Reward =  -2492.6604\n",
      "Episode  526 : Reward =  -2879.8044\n",
      "Episode  527 : Reward =  -2073.26\n",
      "Episode  528 : Reward =  -2869.0034\n",
      "Episode  529 : Reward =  -2471.6094\n",
      "Episode  530 : Reward =  -4869.0127\n",
      "Episode  531 : Reward =  -3244.363\n",
      "Episode  532 : Reward =  -2795.4333\n",
      "Episode  533 : Reward =  -3959.4463\n",
      "Episode  534 : Reward =  -2037.1072\n",
      "Episode  535 : Reward =  -2445.0405\n",
      "Episode  536 : Reward =  -3604.3447\n",
      "Episode  537 : Reward =  -2425.9827\n",
      "Episode  538 : Reward =  -1607.3306\n",
      "Saving better model at episode 538 with reward -1607.33056640625\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.365\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  539 : Reward =  -1549.0541\n",
      "Saving better model at episode 539 with reward -1549.0540771484375\n",
      "Episode  540 : Reward =  -3195.5151\n",
      "Episode  541 : Reward =  -1611.5955\n",
      "Episode  542 : Reward =  -2404.4248\n",
      "Episode  543 : Reward =  -2382.179\n",
      "Episode  544 : Reward =  -1498.1498\n",
      "Saving better model at episode 544 with reward -1498.1497802734375\n",
      "Episode  545 : Reward =  -2026.8315\n",
      "Episode  546 : Reward =  -1964.3752\n",
      "Episode  547 : Reward =  -1573.5582\n",
      "Episode  548 : Reward =  -2052.539\n",
      "Episode  549 : Reward =  -1624.9231\n",
      "Episode  550 : Reward =  -1587.0028\n",
      "Episode  551 : Reward =  -4837.09\n",
      "Episode  552 : Reward =  -1983.237\n",
      "Episode  553 : Reward =  -1648.0436\n",
      "Episode  554 : Reward =  -3282.9001\n",
      "Episode  555 : Reward =  -2874.4895\n",
      "Episode  556 : Reward =  -4060.4753\n",
      "Episode  557 : Reward =  -1596.7528\n",
      "Episode  558 : Reward =  -1624.1749\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.36\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  559 : Reward =  -2091.8542\n",
      "Episode  560 : Reward =  -1561.5969\n",
      "Episode  561 : Reward =  -1650.4446\n",
      "Episode  562 : Reward =  -1645.5349\n",
      "Episode  563 : Reward =  -1688.8617\n",
      "Episode  564 : Reward =  -1565.5656\n",
      "Episode  565 : Reward =  -1630.8287\n",
      "Episode  566 : Reward =  -1523.5991\n",
      "Episode  567 : Reward =  -3338.369\n",
      "Episode  568 : Reward =  -4029.284\n",
      "Episode  569 : Reward =  -1592.7678\n",
      "Episode  570 : Reward =  -1595.1268\n",
      "Episode  571 : Reward =  -1628.8699\n",
      "Episode  572 : Reward =  -1661.5745\n",
      "Episode  573 : Reward =  -1641.8575\n",
      "Episode  574 : Reward =  -1706.9255\n",
      "Episode  575 : Reward =  -1676.2614\n",
      "Episode  576 : Reward =  -1575.4391\n",
      "Episode  577 : Reward =  -1561.8468\n",
      "Episode  578 : Reward =  -1584.5485\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.355\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  579 : Reward =  -1594.4841\n",
      "Episode  580 : Reward =  -1502.3806\n",
      "Episode  581 : Reward =  -2413.606\n",
      "Episode  582 : Reward =  -1613.5212\n",
      "Episode  583 : Reward =  -1451.268\n",
      "Saving better model at episode 583 with reward -1451.2679443359375\n",
      "Episode  584 : Reward =  -2418.5115\n",
      "Episode  585 : Reward =  -1601.8552\n",
      "Episode  586 : Reward =  -1472.6604\n",
      "Episode  587 : Reward =  -1673.6224\n",
      "Episode  588 : Reward =  -1537.1992\n",
      "Episode  589 : Reward =  -1556.9342\n",
      "Episode  590 : Reward =  -2434.3923\n",
      "Episode  591 : Reward =  -1525.7443\n",
      "Episode  592 : Reward =  -1492.0819\n",
      "Episode  593 : Reward =  -1494.9706\n",
      "Episode  594 : Reward =  -1471.6243\n",
      "Episode  595 : Reward =  -3624.1062\n",
      "Episode  596 : Reward =  -1531.3474\n",
      "Episode  597 : Reward =  -2801.4412\n",
      "Episode  598 : Reward =  -1655.2114\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.35\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  599 : Reward =  -1645.8088\n",
      "Episode  600 : Reward =  -3694.768\n",
      "Episode  601 : Reward =  -1463.5468\n",
      "Episode  602 : Reward =  -1462.4175\n",
      "Episode  603 : Reward =  -1396.7\n",
      "Saving better model at episode 603 with reward -1396.699951171875\n",
      "Episode  604 : Reward =  -1473.262\n",
      "Episode  605 : Reward =  -1409.0078\n",
      "Episode  606 : Reward =  -1453.7122\n",
      "Episode  607 : Reward =  -2366.28\n",
      "Episode  608 : Reward =  -1418.7883\n",
      "Episode  609 : Reward =  -2327.0955\n",
      "Episode  610 : Reward =  -1435.1241\n",
      "Episode  611 : Reward =  -2759.101\n",
      "Episode  612 : Reward =  -1439.7675\n",
      "Episode  613 : Reward =  -1494.0276\n",
      "Episode  614 : Reward =  -1584.4032\n",
      "Episode  615 : Reward =  -1418.2017\n",
      "Episode  616 : Reward =  -1461.9701\n",
      "Episode  617 : Reward =  -1532.0886\n",
      "Episode  618 : Reward =  -1650.6677\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.345\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  619 : Reward =  -1462.1047\n",
      "Episode  620 : Reward =  -1480.2883\n",
      "Episode  621 : Reward =  -1478.2579\n",
      "Episode  622 : Reward =  -1479.9918\n",
      "Episode  623 : Reward =  -1409.6221\n",
      "Episode  624 : Reward =  -1644.7013\n",
      "Episode  625 : Reward =  -1399.754\n",
      "Episode  626 : Reward =  -1403.336\n",
      "Episode  627 : Reward =  -1458.8975\n",
      "Episode  628 : Reward =  -1442.807\n",
      "Episode  629 : Reward =  -1464.8859\n",
      "Episode  630 : Reward =  -1429.955\n",
      "Episode  631 : Reward =  -1384.641\n",
      "Saving better model at episode 631 with reward -1384.6409912109375\n",
      "Episode  632 : Reward =  -1452.2043\n",
      "Episode  633 : Reward =  -1472.8759\n",
      "Episode  634 : Reward =  -1367.4585\n",
      "Saving better model at episode 634 with reward -1367.45849609375\n",
      "Episode  635 : Reward =  -1398.0244\n",
      "Episode  636 : Reward =  -1452.44\n",
      "Episode  637 : Reward =  -3989.1785\n",
      "Episode  638 : Reward =  -1391.0187\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.34\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  639 : Reward =  -1430.934\n",
      "Episode  640 : Reward =  -1433.5183\n",
      "Episode  641 : Reward =  -1453.4933\n",
      "Episode  642 : Reward =  -1459.6672\n",
      "Episode  643 : Reward =  -1430.3198\n",
      "Episode  644 : Reward =  -1422.9493\n",
      "Episode  645 : Reward =  -1437.3336\n",
      "Episode  646 : Reward =  -1467.4548\n",
      "Episode  647 : Reward =  -1406.7452\n",
      "Episode  648 : Reward =  -1453.2196\n",
      "Episode  649 : Reward =  -1452.9543\n",
      "Episode  650 : Reward =  -1401.3923\n",
      "Episode  651 : Reward =  -1503.4757\n",
      "Episode  652 : Reward =  -1521.1598\n",
      "Episode  653 : Reward =  -1442.0859\n",
      "Episode  654 : Reward =  -1372.7852\n",
      "Episode  655 : Reward =  -2003.9744\n",
      "Episode  656 : Reward =  -1359.1969\n",
      "Saving better model at episode 656 with reward -1359.1968994140625\n",
      "Episode  657 : Reward =  -1387.3894\n",
      "Episode  658 : Reward =  -1414.4064\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.335\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  659 : Reward =  -1519.3873\n",
      "Episode  660 : Reward =  -1370.5966\n",
      "Episode  661 : Reward =  -1416.1913\n",
      "Episode  662 : Reward =  -1393.8113\n",
      "Episode  663 : Reward =  -1383.1244\n",
      "Episode  664 : Reward =  -1418.5377\n",
      "Episode  665 : Reward =  -1339.7189\n",
      "Saving better model at episode 665 with reward -1339.7188720703125\n",
      "Episode  666 : Reward =  -1461.1521\n",
      "Episode  667 : Reward =  -1392.7747\n",
      "Episode  668 : Reward =  -1404.3206\n",
      "Episode  669 : Reward =  -1571.5131\n",
      "Episode  670 : Reward =  -1377.1506\n",
      "Episode  671 : Reward =  -1447.482\n",
      "Episode  672 : Reward =  -1356.9832\n",
      "Episode  673 : Reward =  -1517.3248\n",
      "Episode  674 : Reward =  -1419.1301\n",
      "Episode  675 : Reward =  -1427.581\n",
      "Episode  676 : Reward =  -1355.8583\n",
      "Episode  677 : Reward =  -1377.7203\n",
      "Episode  678 : Reward =  -1368.1278\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.33\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  679 : Reward =  -1569.7488\n",
      "Episode  680 : Reward =  -1390.8907\n",
      "Episode  681 : Reward =  -1402.0768\n",
      "Episode  682 : Reward =  -1407.6487\n",
      "Episode  683 : Reward =  -1372.2637\n",
      "Episode  684 : Reward =  -1351.3141\n",
      "Episode  685 : Reward =  -1527.4084\n",
      "Episode  686 : Reward =  -1443.0503\n",
      "Episode  687 : Reward =  -1364.4635\n",
      "Episode  688 : Reward =  -1344.5668\n",
      "Episode  689 : Reward =  -1358.6727\n",
      "Episode  690 : Reward =  -1378.7179\n",
      "Episode  691 : Reward =  -1427.4912\n",
      "Episode  692 : Reward =  -1865.3342\n",
      "Episode  693 : Reward =  -1940.2151\n",
      "Episode  694 : Reward =  -1365.3322\n",
      "Episode  695 : Reward =  -1384.3762\n",
      "Episode  696 : Reward =  -2675.7856\n",
      "Episode  697 : Reward =  -1924.8389\n",
      "Episode  698 : Reward =  -1348.2177\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.325\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  699 : Reward =  -2313.982\n",
      "Episode  700 : Reward =  -1346.5845\n",
      "Episode  701 : Reward =  -1376.3698\n",
      "Episode  702 : Reward =  -1432.5344\n",
      "Episode  703 : Reward =  -3825.3853\n",
      "Episode  704 : Reward =  -1421.4932\n",
      "Episode  705 : Reward =  -1355.9955\n",
      "Episode  706 : Reward =  -1356.9865\n",
      "Episode  707 : Reward =  -1881.2968\n",
      "Episode  708 : Reward =  -3882.5312\n",
      "Episode  709 : Reward =  -1288.2188\n",
      "Saving better model at episode 709 with reward -1288.21875\n",
      "Episode  710 : Reward =  -1379.5278\n",
      "Episode  711 : Reward =  -1315.9287\n",
      "Episode  712 : Reward =  -1387.5736\n",
      "Episode  713 : Reward =  -1335.0745\n",
      "Episode  714 : Reward =  -1391.3683\n",
      "Episode  715 : Reward =  -1426.7357\n",
      "Episode  716 : Reward =  -1515.877\n",
      "Episode  717 : Reward =  -1345.6416\n",
      "Episode  718 : Reward =  -1362.5594\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.32\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  719 : Reward =  -1366.023\n",
      "Episode  720 : Reward =  -1358.4021\n",
      "Episode  721 : Reward =  -1344.139\n",
      "Episode  722 : Reward =  -1852.7098\n",
      "Episode  723 : Reward =  -1372.8546\n",
      "Episode  724 : Reward =  -1243.3196\n",
      "Saving better model at episode 724 with reward -1243.319580078125\n",
      "Episode  725 : Reward =  -1278.31\n",
      "Episode  726 : Reward =  -2265.8872\n",
      "Episode  727 : Reward =  -1321.9093\n",
      "Episode  728 : Reward =  -1390.8916\n",
      "Episode  729 : Reward =  -2273.6323\n",
      "Episode  730 : Reward =  -1343.1213\n",
      "Episode  731 : Reward =  -1360.1565\n",
      "Episode  732 : Reward =  -1868.2097\n",
      "Episode  733 : Reward =  -1285.381\n",
      "Episode  734 : Reward =  -1351.3339\n",
      "Episode  735 : Reward =  -1226.6155\n",
      "Saving better model at episode 735 with reward -1226.615478515625\n",
      "Episode  736 : Reward =  -1277.4136\n",
      "Episode  737 : Reward =  -1845.3507\n",
      "Episode  738 : Reward =  -1279.2494\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.315\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  739 : Reward =  -1281.4044\n",
      "Episode  740 : Reward =  -1288.2357\n",
      "Episode  741 : Reward =  -1260.0182\n",
      "Episode  742 : Reward =  -1330.4082\n",
      "Episode  743 : Reward =  -5077.7803\n",
      "Episode  744 : Reward =  -1337.1154\n",
      "Episode  745 : Reward =  -1240.0205\n",
      "Episode  746 : Reward =  -2214.5476\n",
      "Episode  747 : Reward =  -1250.3383\n",
      "Episode  748 : Reward =  -1329.1824\n",
      "Episode  749 : Reward =  -1779.2064\n",
      "Episode  750 : Reward =  -2220.636\n",
      "Episode  751 : Reward =  -1276.9252\n",
      "Episode  752 : Reward =  -1255.6113\n",
      "Episode  753 : Reward =  -1271.1652\n",
      "Episode  754 : Reward =  -1392.577\n",
      "Episode  755 : Reward =  -1309.5396\n",
      "Episode  756 : Reward =  -3372.2495\n",
      "Episode  757 : Reward =  -1336.9977\n",
      "Episode  758 : Reward =  -1263.0306\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.31\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  759 : Reward =  -1383.5256\n",
      "Episode  760 : Reward =  -1417.0468\n",
      "Episode  761 : Reward =  -1243.5049\n",
      "Episode  762 : Reward =  -2582.4587\n",
      "Episode  763 : Reward =  -1245.786\n",
      "Episode  764 : Reward =  -1271.5377\n",
      "Episode  765 : Reward =  -1256.9071\n",
      "Episode  766 : Reward =  -1848.5537\n",
      "Episode  767 : Reward =  -1315.5074\n",
      "Episode  768 : Reward =  -1234.0046\n",
      "Episode  769 : Reward =  -1267.1506\n",
      "Episode  770 : Reward =  -1319.4607\n",
      "Episode  771 : Reward =  -1262.6902\n",
      "Episode  772 : Reward =  -2676.2512\n",
      "Episode  773 : Reward =  -1294.8618\n",
      "Episode  774 : Reward =  -1814.9082\n",
      "Episode  775 : Reward =  -1232.7131\n",
      "Episode  776 : Reward =  -2200.339\n",
      "Episode  777 : Reward =  -1383.5289\n",
      "Episode  778 : Reward =  -1272.1174\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.305\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  779 : Reward =  -1339.3823\n",
      "Episode  780 : Reward =  -1416.166\n",
      "Episode  781 : Reward =  -1266.9933\n",
      "Episode  782 : Reward =  -1241.7057\n",
      "Episode  783 : Reward =  -1373.2949\n",
      "Episode  784 : Reward =  -1287.5917\n",
      "Episode  785 : Reward =  -1239.8297\n",
      "Episode  786 : Reward =  -2280.8855\n",
      "Episode  787 : Reward =  -1806.1923\n",
      "Episode  788 : Reward =  -2689.61\n",
      "Episode  789 : Reward =  -1352.7933\n",
      "Episode  790 : Reward =  -1368.5242\n",
      "Episode  791 : Reward =  -1498.1478\n",
      "Episode  792 : Reward =  -1276.0435\n",
      "Episode  793 : Reward =  -1325.1927\n",
      "Episode  794 : Reward =  -2040.077\n",
      "Episode  795 : Reward =  -1317.1543\n",
      "Episode  796 : Reward =  -2766.4775\n",
      "Episode  797 : Reward =  -1507.5541\n",
      "Episode  798 : Reward =  -1371.3087\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.3\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  799 : Reward =  -1482.2197\n",
      "Episode  800 : Reward =  -1278.8027\n",
      "Episode  801 : Reward =  -1460.221\n",
      "Episode  802 : Reward =  -1380.4407\n",
      "Episode  803 : Reward =  -1481.211\n",
      "Episode  804 : Reward =  -1323.9912\n",
      "Episode  805 : Reward =  -1362.8103\n",
      "Episode  806 : Reward =  -1393.0577\n",
      "Episode  807 : Reward =  -1365.2535\n",
      "Episode  808 : Reward =  -1352.6941\n",
      "Episode  809 : Reward =  -3609.545\n",
      "Episode  810 : Reward =  -1389.5012\n",
      "Episode  811 : Reward =  -1499.1653\n",
      "Episode  812 : Reward =  -1432.0385\n",
      "Episode  813 : Reward =  -1399.0642\n",
      "Episode  814 : Reward =  -2752.912\n",
      "Episode  815 : Reward =  -1953.0088\n",
      "Episode  816 : Reward =  -1377.0758\n",
      "Episode  817 : Reward =  -1305.4064\n",
      "Episode  818 : Reward =  -2435.7961\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.295\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  819 : Reward =  -1493.458\n",
      "Episode  820 : Reward =  -1267.1038\n",
      "Episode  821 : Reward =  -1326.0322\n",
      "Episode  822 : Reward =  -3581.683\n",
      "Episode  823 : Reward =  -1399.5803\n",
      "Episode  824 : Reward =  -1412.6602\n",
      "Episode  825 : Reward =  -1332.4941\n",
      "Episode  826 : Reward =  -1398.7698\n",
      "Episode  827 : Reward =  -1307.518\n",
      "Episode  828 : Reward =  -1450.384\n",
      "Episode  829 : Reward =  -1520.2567\n",
      "Episode  830 : Reward =  -1372.3121\n",
      "Episode  831 : Reward =  -1346.556\n",
      "Episode  832 : Reward =  -1408.4811\n",
      "Episode  833 : Reward =  -1198.5721\n",
      "Saving better model at episode 833 with reward -1198.5721435546875\n",
      "Episode  834 : Reward =  -1260.8098\n",
      "Episode  835 : Reward =  -1272.1224\n",
      "Episode  836 : Reward =  -1232.3037\n",
      "Episode  837 : Reward =  -1355.3866\n",
      "Episode  838 : Reward =  -1484.2117\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.29\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  839 : Reward =  -3548.3118\n",
      "Episode  840 : Reward =  -1490.4609\n",
      "Episode  841 : Reward =  -1525.759\n",
      "Episode  842 : Reward =  -1213.3159\n",
      "Episode  843 : Reward =  -1298.5918\n",
      "Episode  844 : Reward =  -1334.6101\n",
      "Episode  845 : Reward =  -1333.0858\n",
      "Episode  846 : Reward =  -1312.3197\n",
      "Episode  847 : Reward =  -1862.9166\n",
      "Episode  848 : Reward =  -1341.516\n",
      "Episode  849 : Reward =  -1943.371\n",
      "Episode  850 : Reward =  -1312.9772\n",
      "Episode  851 : Reward =  -1298.071\n",
      "Episode  852 : Reward =  -1307.7485\n",
      "Episode  853 : Reward =  -1803.1439\n",
      "Episode  854 : Reward =  -3103.4429\n",
      "Episode  855 : Reward =  -1439.4781\n",
      "Episode  856 : Reward =  -1415.2682\n",
      "Episode  857 : Reward =  -1808.3785\n",
      "Episode  858 : Reward =  -1406.4788\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.285\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  859 : Reward =  -1320.7871\n",
      "Episode  860 : Reward =  -1286.2563\n",
      "Episode  861 : Reward =  -1366.8533\n",
      "Episode  862 : Reward =  -1355.4603\n",
      "Episode  863 : Reward =  -1385.714\n",
      "Episode  864 : Reward =  -1284.6432\n",
      "Episode  865 : Reward =  -1945.7858\n",
      "Episode  866 : Reward =  -1307.1715\n",
      "Episode  867 : Reward =  -1292.165\n",
      "Episode  868 : Reward =  -1937.503\n",
      "Episode  869 : Reward =  -1349.892\n",
      "Episode  870 : Reward =  -1214.8619\n",
      "Episode  871 : Reward =  -1385.1649\n",
      "Episode  872 : Reward =  -1364.466\n",
      "Episode  873 : Reward =  -2412.684\n",
      "Episode  874 : Reward =  -1282.6581\n",
      "Episode  875 : Reward =  -1286.5671\n",
      "Episode  876 : Reward =  -1339.0521\n",
      "Episode  877 : Reward =  -1450.4844\n",
      "Episode  878 : Reward =  -1316.011\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.28\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  879 : Reward =  -1304.6897\n",
      "Episode  880 : Reward =  -1365.7582\n",
      "Episode  881 : Reward =  -1451.7839\n",
      "Episode  882 : Reward =  -1247.1241\n",
      "Episode  883 : Reward =  -1342.1833\n",
      "Episode  884 : Reward =  -2298.4626\n",
      "Episode  885 : Reward =  -1205.353\n",
      "Episode  886 : Reward =  -1263.5228\n",
      "Episode  887 : Reward =  -2251.306\n",
      "Episode  888 : Reward =  -1348.4902\n",
      "Episode  889 : Reward =  -1299.4293\n",
      "Episode  890 : Reward =  -1502.7042\n",
      "Episode  891 : Reward =  -1269.4277\n",
      "Episode  892 : Reward =  -1979.4841\n",
      "Episode  893 : Reward =  -1374.4801\n",
      "Episode  894 : Reward =  -1956.2373\n",
      "Episode  895 : Reward =  -2739.4011\n",
      "Episode  896 : Reward =  -1295.5662\n",
      "Episode  897 : Reward =  -1332.7673\n",
      "Episode  898 : Reward =  -1985.2816\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.275\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  899 : Reward =  -2691.4963\n",
      "Episode  900 : Reward =  -1956.5541\n",
      "Episode  901 : Reward =  -1576.0779\n",
      "Episode  902 : Reward =  -1592.9716\n",
      "Episode  903 : Reward =  -1432.8679\n",
      "Episode  904 : Reward =  -1459.8676\n",
      "Episode  905 : Reward =  -1505.772\n",
      "Episode  906 : Reward =  -1460.2966\n",
      "Episode  907 : Reward =  -1325.7725\n",
      "Episode  908 : Reward =  -1525.3512\n",
      "Episode  909 : Reward =  -1382.4492\n",
      "Episode  910 : Reward =  -1522.8864\n",
      "Episode  911 : Reward =  -1469.3654\n",
      "Episode  912 : Reward =  -1390.0854\n",
      "Episode  913 : Reward =  -1671.7917\n",
      "Episode  914 : Reward =  -1445.3969\n",
      "Episode  915 : Reward =  -1344.082\n",
      "Episode  916 : Reward =  -1477.9191\n",
      "Episode  917 : Reward =  -1369.7126\n",
      "Episode  918 : Reward =  -1407.4067\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.27\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  919 : Reward =  -1373.9337\n",
      "Episode  920 : Reward =  -1462.4786\n",
      "Episode  921 : Reward =  -1572.5509\n",
      "Episode  922 : Reward =  -1474.741\n",
      "Episode  923 : Reward =  -1956.4569\n",
      "Episode  924 : Reward =  -1313.158\n",
      "Episode  925 : Reward =  -1284.6814\n",
      "Episode  926 : Reward =  -1385.4937\n",
      "Episode  927 : Reward =  -1340.3052\n",
      "Episode  928 : Reward =  -1509.7135\n",
      "Episode  929 : Reward =  -2569.6665\n",
      "Episode  930 : Reward =  -1360.4076\n",
      "Episode  931 : Reward =  -1240.7605\n",
      "Episode  932 : Reward =  -1235.0917\n",
      "Episode  933 : Reward =  -1456.878\n",
      "Episode  934 : Reward =  -1328.6034\n",
      "Episode  935 : Reward =  -1574.3081\n",
      "Episode  936 : Reward =  -1344.4083\n",
      "Episode  937 : Reward =  -1238.4097\n",
      "Episode  938 : Reward =  -1171.1085\n",
      "Saving better model at episode 938 with reward -1171.1085205078125\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.265\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  939 : Reward =  -1294.3622\n",
      "Episode  940 : Reward =  -1557.4485\n",
      "Episode  941 : Reward =  -1296.0511\n",
      "Episode  942 : Reward =  -1282.0681\n",
      "Episode  943 : Reward =  -2045.2454\n",
      "Episode  944 : Reward =  -1683.7152\n",
      "Episode  945 : Reward =  -1364.0094\n",
      "Episode  946 : Reward =  -1286.199\n",
      "Episode  947 : Reward =  -1414.3241\n",
      "Episode  948 : Reward =  -1352.0691\n",
      "Episode  949 : Reward =  -1465.9757\n",
      "Episode  950 : Reward =  -1571.1688\n",
      "Episode  951 : Reward =  -1434.8727\n",
      "Episode  952 : Reward =  -1169.0796\n",
      "Saving better model at episode 952 with reward -1169.07958984375\n",
      "Episode  953 : Reward =  -1221.4243\n",
      "Episode  954 : Reward =  -1186.3385\n",
      "Episode  955 : Reward =  -1193.1873\n",
      "Episode  956 : Reward =  -1272.9955\n",
      "Episode  957 : Reward =  -1337.3928\n",
      "Episode  958 : Reward =  -1303.5609\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.26\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  959 : Reward =  -1327.8372\n",
      "Episode  960 : Reward =  -1222.3617\n",
      "Episode  961 : Reward =  -1451.422\n",
      "Episode  962 : Reward =  -1293.8733\n",
      "Episode  963 : Reward =  -1313.3322\n",
      "Episode  964 : Reward =  -1998.2308\n",
      "Episode  965 : Reward =  -1346.3726\n",
      "Episode  966 : Reward =  -1909.522\n",
      "Episode  967 : Reward =  -1278.0964\n",
      "Episode  968 : Reward =  -1358.9127\n",
      "Episode  969 : Reward =  -1357.8688\n",
      "Episode  970 : Reward =  -1296.4915\n",
      "Episode  971 : Reward =  -1415.2561\n",
      "Episode  972 : Reward =  -1310.161\n",
      "Episode  973 : Reward =  -1236.5664\n",
      "Episode  974 : Reward =  -1233.8062\n",
      "Episode  975 : Reward =  -1413.9426\n",
      "Episode  976 : Reward =  -1702.8181\n",
      "Episode  977 : Reward =  -1263.9895\n",
      "Episode  978 : Reward =  -1261.3988\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.255\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  979 : Reward =  -1982.3004\n",
      "Episode  980 : Reward =  -1449.5049\n",
      "Episode  981 : Reward =  -1304.2277\n",
      "Episode  982 : Reward =  -1255.0006\n",
      "Episode  983 : Reward =  -1282.6329\n",
      "Episode  984 : Reward =  -1462.6886\n",
      "Episode  985 : Reward =  -1269.6501\n",
      "Episode  986 : Reward =  -1425.6868\n",
      "Episode  987 : Reward =  -1399.1187\n",
      "Episode  988 : Reward =  -1331.2198\n",
      "Episode  989 : Reward =  -1205.3271\n",
      "Episode  990 : Reward =  -1307.8792\n",
      "Episode  991 : Reward =  -1227.3611\n",
      "Episode  992 : Reward =  -1250.7422\n",
      "Episode  993 : Reward =  -1577.5536\n",
      "Episode  994 : Reward =  -1223.6948\n",
      "Episode  995 : Reward =  -1555.9426\n",
      "Episode  996 : Reward =  -1843.9728\n",
      "Episode  997 : Reward =  -1196.5615\n",
      "Episode  998 : Reward =  -1273.109\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.25\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  999 : Reward =  -1194.9492\n",
      "Episode  1000 : Reward =  -1204.8127\n",
      "Episode  1001 : Reward =  -1211.9656\n",
      "Episode  1002 : Reward =  -1549.3354\n",
      "Episode  1003 : Reward =  -1248.5205\n",
      "Episode  1004 : Reward =  -1472.521\n",
      "Episode  1005 : Reward =  -2081.53\n",
      "Episode  1006 : Reward =  -1214.0223\n",
      "Episode  1007 : Reward =  -1174.7593\n",
      "Episode  1008 : Reward =  -1299.8358\n",
      "Episode  1009 : Reward =  -1370.3362\n",
      "Episode  1010 : Reward =  -1290.5455\n",
      "Episode  1011 : Reward =  -1308.0316\n",
      "Episode  1012 : Reward =  -1539.3047\n",
      "Episode  1013 : Reward =  -1197.7255\n",
      "Episode  1014 : Reward =  -1447.5646\n",
      "Episode  1015 : Reward =  -1226.3986\n",
      "Episode  1016 : Reward =  -1396.757\n",
      "Episode  1017 : Reward =  -1272.4747\n",
      "Episode  1018 : Reward =  -1559.4962\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.245\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1019 : Reward =  -1260.4718\n",
      "Episode  1020 : Reward =  -1362.6345\n",
      "Episode  1021 : Reward =  -1415.9521\n",
      "Episode  1022 : Reward =  -1208.2393\n",
      "Episode  1023 : Reward =  -1331.4646\n",
      "Episode  1024 : Reward =  -1574.8002\n",
      "Episode  1025 : Reward =  -1876.6296\n",
      "Episode  1026 : Reward =  -1290.5792\n",
      "Episode  1027 : Reward =  -1249.6635\n",
      "Episode  1028 : Reward =  -1448.7537\n",
      "Episode  1029 : Reward =  -1217.6699\n",
      "Episode  1030 : Reward =  -1241.9113\n",
      "Episode  1031 : Reward =  -1339.5282\n",
      "Episode  1032 : Reward =  -1497.6229\n",
      "Episode  1033 : Reward =  -1249.4111\n",
      "Episode  1034 : Reward =  -1334.7133\n",
      "Episode  1035 : Reward =  -1353.79\n",
      "Episode  1036 : Reward =  -1337.9905\n",
      "Episode  1037 : Reward =  -1196.8511\n",
      "Episode  1038 : Reward =  -1327.0336\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.24\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1039 : Reward =  -1287.0187\n",
      "Episode  1040 : Reward =  -1271.4371\n",
      "Episode  1041 : Reward =  -1163.0593\n",
      "Saving better model at episode 1041 with reward -1163.059326171875\n",
      "Episode  1042 : Reward =  -1557.2224\n",
      "Episode  1043 : Reward =  -1411.1248\n",
      "Episode  1044 : Reward =  -1167.8291\n",
      "Episode  1045 : Reward =  -1280.0562\n",
      "Episode  1046 : Reward =  -1596.7926\n",
      "Episode  1047 : Reward =  -1474.9836\n",
      "Episode  1048 : Reward =  -1209.5432\n",
      "Episode  1049 : Reward =  -1170.9939\n",
      "Episode  1050 : Reward =  -1342.8293\n",
      "Episode  1051 : Reward =  -1274.3616\n",
      "Episode  1052 : Reward =  -1242.9657\n",
      "Episode  1053 : Reward =  -1278.9197\n",
      "Episode  1054 : Reward =  -1906.2007\n",
      "Episode  1055 : Reward =  -1129.0869\n",
      "Saving better model at episode 1055 with reward -1129.0869140625\n",
      "Episode  1056 : Reward =  -1309.0743\n",
      "Episode  1057 : Reward =  -1431.295\n",
      "Episode  1058 : Reward =  -1190.5651\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.235\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1059 : Reward =  -1250.3123\n",
      "Episode  1060 : Reward =  -1266.2754\n",
      "Episode  1061 : Reward =  -1918.8229\n",
      "Episode  1062 : Reward =  -1883.8782\n",
      "Episode  1063 : Reward =  -1135.5938\n",
      "Episode  1064 : Reward =  -1338.7954\n",
      "Episode  1065 : Reward =  -1423.7853\n",
      "Episode  1066 : Reward =  -1208.3099\n",
      "Episode  1067 : Reward =  -1128.6958\n",
      "Saving better model at episode 1067 with reward -1128.69580078125\n",
      "Episode  1068 : Reward =  -1282.314\n",
      "Episode  1069 : Reward =  -1378.0747\n",
      "Episode  1070 : Reward =  -1234.2378\n",
      "Episode  1071 : Reward =  -1249.5986\n",
      "Episode  1072 : Reward =  -1254.3085\n",
      "Episode  1073 : Reward =  -1197.2031\n",
      "Episode  1074 : Reward =  -1253.2979\n",
      "Episode  1075 : Reward =  -1215.7336\n",
      "Episode  1076 : Reward =  -1159.121\n",
      "Episode  1077 : Reward =  -1232.0897\n",
      "Episode  1078 : Reward =  -2244.9124\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.23\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1079 : Reward =  -1165.8231\n",
      "Episode  1080 : Reward =  -1226.6031\n",
      "Episode  1081 : Reward =  -1112.9963\n",
      "Saving better model at episode 1081 with reward -1112.996337890625\n",
      "Episode  1082 : Reward =  -1409.2721\n",
      "Episode  1083 : Reward =  -1921.5177\n",
      "Episode  1084 : Reward =  -1183.1732\n",
      "Episode  1085 : Reward =  -1170.5083\n",
      "Episode  1086 : Reward =  -1208.1871\n",
      "Episode  1087 : Reward =  -1227.9452\n",
      "Episode  1088 : Reward =  -1217.2225\n",
      "Episode  1089 : Reward =  -1133.6912\n",
      "Episode  1090 : Reward =  -1265.5035\n",
      "Episode  1091 : Reward =  -1238.3456\n",
      "Episode  1092 : Reward =  -1126.9692\n",
      "Episode  1093 : Reward =  -1171.8046\n",
      "Episode  1094 : Reward =  -1219.099\n",
      "Episode  1095 : Reward =  -1312.5105\n",
      "Episode  1096 : Reward =  -1144.6154\n",
      "Episode  1097 : Reward =  -1325.7551\n",
      "Episode  1098 : Reward =  -2612.2698\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.225\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1099 : Reward =  -2590.9885\n",
      "Episode  1100 : Reward =  -3724.1626\n",
      "Episode  1101 : Reward =  -1698.0674\n",
      "Episode  1102 : Reward =  -1831.4812\n",
      "Episode  1103 : Reward =  -1175.0468\n",
      "Episode  1104 : Reward =  -1148.1948\n",
      "Episode  1105 : Reward =  -1319.1854\n",
      "Episode  1106 : Reward =  -1304.2347\n",
      "Episode  1107 : Reward =  -1237.7526\n",
      "Episode  1108 : Reward =  -1251.3379\n",
      "Episode  1109 : Reward =  -1242.3805\n",
      "Episode  1110 : Reward =  -1270.2706\n",
      "Episode  1111 : Reward =  -1302.8435\n",
      "Episode  1112 : Reward =  -1297.6971\n",
      "Episode  1113 : Reward =  -1262.4167\n",
      "Episode  1114 : Reward =  -1354.1941\n",
      "Episode  1115 : Reward =  -1244.7067\n",
      "Episode  1116 : Reward =  -1277.5867\n",
      "Episode  1117 : Reward =  -1867.5233\n",
      "Episode  1118 : Reward =  -1172.5562\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.22\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1119 : Reward =  -1339.8242\n",
      "Episode  1120 : Reward =  -1338.5417\n",
      "Episode  1121 : Reward =  -2748.7192\n",
      "Episode  1122 : Reward =  -1257.2893\n",
      "Episode  1123 : Reward =  -1421.5155\n",
      "Episode  1124 : Reward =  -1333.7273\n",
      "Episode  1125 : Reward =  -1147.8481\n",
      "Episode  1126 : Reward =  -1152.5463\n",
      "Episode  1127 : Reward =  -1363.5073\n",
      "Episode  1128 : Reward =  -1219.5037\n",
      "Episode  1129 : Reward =  -1295.5171\n",
      "Episode  1130 : Reward =  -1065.2112\n",
      "Saving better model at episode 1130 with reward -1065.211181640625\n",
      "Episode  1131 : Reward =  -1251.7448\n",
      "Episode  1132 : Reward =  -1341.2529\n",
      "Episode  1133 : Reward =  -1118.0496\n",
      "Episode  1134 : Reward =  -1219.2264\n",
      "Episode  1135 : Reward =  -1054.9006\n",
      "Saving better model at episode 1135 with reward -1054.900634765625\n",
      "Episode  1136 : Reward =  -1270.6193\n",
      "Episode  1137 : Reward =  -1184.143\n",
      "Episode  1138 : Reward =  -1225.6144\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.215\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1139 : Reward =  -1204.4216\n",
      "Episode  1140 : Reward =  -1115.313\n",
      "Episode  1141 : Reward =  -1356.5134\n",
      "Episode  1142 : Reward =  -1094.7197\n",
      "Episode  1143 : Reward =  -1204.825\n",
      "Episode  1144 : Reward =  -1233.79\n",
      "Episode  1145 : Reward =  -1085.1265\n",
      "Episode  1146 : Reward =  -1456.317\n",
      "Episode  1147 : Reward =  -1383.8909\n",
      "Episode  1148 : Reward =  -1305.4615\n",
      "Episode  1149 : Reward =  -1296.8607\n",
      "Episode  1150 : Reward =  -1316.2189\n",
      "Episode  1151 : Reward =  -1307.2976\n",
      "Episode  1152 : Reward =  -1165.1334\n",
      "Episode  1153 : Reward =  -1257.3403\n",
      "Episode  1154 : Reward =  -1336.0818\n",
      "Episode  1155 : Reward =  -1178.3132\n",
      "Episode  1156 : Reward =  -1308.9756\n",
      "Episode  1157 : Reward =  -1179.1538\n",
      "Episode  1158 : Reward =  -1298.454\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.21\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1159 : Reward =  -1273.249\n",
      "Episode  1160 : Reward =  -1166.1233\n",
      "Episode  1161 : Reward =  -1313.8855\n",
      "Episode  1162 : Reward =  -1291.3475\n",
      "Episode  1163 : Reward =  -1345.262\n",
      "Episode  1164 : Reward =  -1198.4431\n",
      "Episode  1165 : Reward =  -1140.7744\n",
      "Episode  1166 : Reward =  -1301.8466\n",
      "Episode  1167 : Reward =  -1398.0432\n",
      "Episode  1168 : Reward =  -1174.4468\n",
      "Episode  1169 : Reward =  -1231.7638\n",
      "Episode  1170 : Reward =  -1164.515\n",
      "Episode  1171 : Reward =  -1128.8196\n",
      "Episode  1172 : Reward =  -1381.9358\n",
      "Episode  1173 : Reward =  -1281.225\n",
      "Episode  1174 : Reward =  -1222.7983\n",
      "Episode  1175 : Reward =  -1195.1531\n",
      "Episode  1176 : Reward =  -1260.3617\n",
      "Episode  1177 : Reward =  -1195.6927\n",
      "Episode  1178 : Reward =  -1142.4784\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.205\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1179 : Reward =  -1253.703\n",
      "Episode  1180 : Reward =  -1397.97\n",
      "Episode  1181 : Reward =  -1198.5374\n",
      "Episode  1182 : Reward =  -1399.8014\n",
      "Episode  1183 : Reward =  -1416.6423\n",
      "Episode  1184 : Reward =  -1165.4697\n",
      "Episode  1185 : Reward =  -1224.789\n",
      "Episode  1186 : Reward =  -1145.6434\n",
      "Episode  1187 : Reward =  -1168.5232\n",
      "Episode  1188 : Reward =  -1295.3014\n",
      "Episode  1189 : Reward =  -1229.0532\n",
      "Episode  1190 : Reward =  -1287.771\n",
      "Episode  1191 : Reward =  -1180.6188\n",
      "Episode  1192 : Reward =  -1424.5205\n",
      "Episode  1193 : Reward =  -1194.1968\n",
      "Episode  1194 : Reward =  -1197.8073\n",
      "Episode  1195 : Reward =  -1227.1388\n",
      "Episode  1196 : Reward =  -1228.709\n",
      "Episode  1197 : Reward =  -1239.6443\n",
      "Episode  1198 : Reward =  -1181.1188\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.2\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1199 : Reward =  -1204.3633\n",
      "Episode  1200 : Reward =  -1240.1578\n",
      "Episode  1201 : Reward =  -1177.9651\n",
      "Episode  1202 : Reward =  -1149.1226\n",
      "Episode  1203 : Reward =  -1197.1989\n",
      "Episode  1204 : Reward =  -1105.085\n",
      "Episode  1205 : Reward =  -1223.6797\n",
      "Episode  1206 : Reward =  -1225.7382\n",
      "Episode  1207 : Reward =  -1141.8939\n",
      "Episode  1208 : Reward =  -2212.1152\n",
      "Episode  1209 : Reward =  -1186.3939\n",
      "Episode  1210 : Reward =  -1208.2894\n",
      "Episode  1211 : Reward =  -1225.4569\n",
      "Episode  1212 : Reward =  -1396.9161\n",
      "Episode  1213 : Reward =  -1760.7058\n",
      "Episode  1214 : Reward =  -1225.397\n",
      "Episode  1215 : Reward =  -2563.7595\n",
      "Episode  1216 : Reward =  -1395.1185\n",
      "Episode  1217 : Reward =  -2270.4268\n",
      "Episode  1218 : Reward =  -1205.0618\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.195\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1219 : Reward =  -1327.9647\n",
      "Episode  1220 : Reward =  -1478.6107\n",
      "Episode  1221 : Reward =  -1237.4214\n",
      "Episode  1222 : Reward =  -1221.633\n",
      "Episode  1223 : Reward =  -1144.1748\n",
      "Episode  1224 : Reward =  -1201.7335\n",
      "Episode  1225 : Reward =  -1341.8179\n",
      "Episode  1226 : Reward =  -1236.5176\n",
      "Episode  1227 : Reward =  -1093.3787\n",
      "Episode  1228 : Reward =  -1769.2421\n",
      "Episode  1229 : Reward =  -1337.2257\n",
      "Episode  1230 : Reward =  -1216.8732\n",
      "Episode  1231 : Reward =  -1109.9045\n",
      "Episode  1232 : Reward =  -1082.4951\n",
      "Episode  1233 : Reward =  -1195.6353\n",
      "Episode  1234 : Reward =  -1254.695\n",
      "Episode  1235 : Reward =  -1262.8525\n",
      "Episode  1236 : Reward =  -1241.2191\n",
      "Episode  1237 : Reward =  -1254.7251\n",
      "Episode  1238 : Reward =  -1156.451\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.19\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1239 : Reward =  -1240.7555\n",
      "Episode  1240 : Reward =  -1690.5613\n",
      "Episode  1241 : Reward =  -1809.1748\n",
      "Episode  1242 : Reward =  -1078.5956\n",
      "Episode  1243 : Reward =  -1246.2323\n",
      "Episode  1244 : Reward =  -1143.2292\n",
      "Episode  1245 : Reward =  -1266.6992\n",
      "Episode  1246 : Reward =  -1335.6606\n",
      "Episode  1247 : Reward =  -1707.2494\n",
      "Episode  1248 : Reward =  -1702.128\n",
      "Episode  1249 : Reward =  -1176.5048\n",
      "Episode  1250 : Reward =  -1380.0563\n",
      "Episode  1251 : Reward =  -2132.2087\n",
      "Episode  1252 : Reward =  -1695.9752\n",
      "Episode  1253 : Reward =  -1222.1875\n",
      "Episode  1254 : Reward =  -1238.5535\n",
      "Episode  1255 : Reward =  -1280.739\n",
      "Episode  1256 : Reward =  -1201.1615\n",
      "Episode  1257 : Reward =  -1178.0198\n",
      "Episode  1258 : Reward =  -2179.4736\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.185\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1259 : Reward =  -1333.4182\n",
      "Episode  1260 : Reward =  -1162.9304\n",
      "Episode  1261 : Reward =  -1457.6537\n",
      "Episode  1262 : Reward =  -1148.5905\n",
      "Episode  1263 : Reward =  -1447.671\n",
      "Episode  1264 : Reward =  -1087.1285\n",
      "Episode  1265 : Reward =  -1140.2583\n",
      "Episode  1266 : Reward =  -1316.786\n",
      "Episode  1267 : Reward =  -1143.6312\n",
      "Episode  1268 : Reward =  -1225.3898\n",
      "Episode  1269 : Reward =  -1233.7344\n",
      "Episode  1270 : Reward =  -1371.5143\n",
      "Episode  1271 : Reward =  -1143.6305\n",
      "Episode  1272 : Reward =  -1650.4976\n",
      "Episode  1273 : Reward =  -1343.9258\n",
      "Episode  1274 : Reward =  -1003.4008\n",
      "Saving better model at episode 1274 with reward -1003.4008178710938\n",
      "Episode  1275 : Reward =  -1167.0808\n",
      "Episode  1276 : Reward =  -1267.7853\n",
      "Episode  1277 : Reward =  -1283.4991\n",
      "Episode  1278 : Reward =  -1247.8198\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.18\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1279 : Reward =  -1262.1257\n",
      "Episode  1280 : Reward =  -1190.3153\n",
      "Episode  1281 : Reward =  -1231.4832\n",
      "Episode  1282 : Reward =  -1348.5311\n",
      "Episode  1283 : Reward =  -2137.21\n",
      "Episode  1284 : Reward =  -1285.5825\n",
      "Episode  1285 : Reward =  -2220.6086\n",
      "Episode  1286 : Reward =  -1661.4698\n",
      "Episode  1287 : Reward =  -1404.2139\n",
      "Episode  1288 : Reward =  -2181.9087\n",
      "Episode  1289 : Reward =  -1681.1279\n",
      "Episode  1290 : Reward =  -1170.353\n",
      "Episode  1291 : Reward =  -1100.6438\n",
      "Episode  1292 : Reward =  -1070.972\n",
      "Episode  1293 : Reward =  -1223.5367\n",
      "Episode  1294 : Reward =  -1114.5712\n",
      "Episode  1295 : Reward =  -1107.586\n",
      "Episode  1296 : Reward =  -1224.9067\n",
      "Episode  1297 : Reward =  -1260.5809\n",
      "Episode  1298 : Reward =  -1330.8262\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.175\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1299 : Reward =  -2154.4622\n",
      "Episode  1300 : Reward =  -1311.6757\n",
      "Episode  1301 : Reward =  -1252.315\n",
      "Episode  1302 : Reward =  -1123.6073\n",
      "Episode  1303 : Reward =  -1369.442\n",
      "Episode  1304 : Reward =  -1691.9357\n",
      "Episode  1305 : Reward =  -1116.8628\n",
      "Episode  1306 : Reward =  -1241.5339\n",
      "Episode  1307 : Reward =  -3312.6733\n",
      "Episode  1308 : Reward =  -2080.143\n",
      "Episode  1309 : Reward =  -2857.9995\n",
      "Episode  1310 : Reward =  -1215.1871\n",
      "Episode  1311 : Reward =  -2067.504\n",
      "Episode  1312 : Reward =  -1229.5013\n",
      "Episode  1313 : Reward =  -1136.391\n",
      "Episode  1314 : Reward =  -1410.2756\n",
      "Episode  1315 : Reward =  -1256.6268\n",
      "Episode  1316 : Reward =  -1158.9766\n",
      "Episode  1317 : Reward =  -1665.6766\n",
      "Episode  1318 : Reward =  -1744.677\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.17\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1319 : Reward =  -1295.6984\n",
      "Episode  1320 : Reward =  -1672.3429\n",
      "Episode  1321 : Reward =  -1612.679\n",
      "Episode  1322 : Reward =  -1156.4495\n",
      "Episode  1323 : Reward =  -1531.1631\n",
      "Episode  1324 : Reward =  -1251.11\n",
      "Episode  1325 : Reward =  -1137.9607\n",
      "Episode  1326 : Reward =  -1283.83\n",
      "Episode  1327 : Reward =  -1182.3892\n",
      "Episode  1328 : Reward =  -1726.9615\n",
      "Episode  1329 : Reward =  -1115.3538\n",
      "Episode  1330 : Reward =  -1189.1476\n",
      "Episode  1331 : Reward =  -1617.2501\n",
      "Episode  1332 : Reward =  -1759.3373\n",
      "Episode  1333 : Reward =  -1288.0956\n",
      "Episode  1334 : Reward =  -1624.8406\n",
      "Episode  1335 : Reward =  -1120.3088\n",
      "Episode  1336 : Reward =  -1429.717\n",
      "Episode  1337 : Reward =  -1157.6163\n",
      "Episode  1338 : Reward =  -1387.8323\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.165\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1339 : Reward =  -2118.3745\n",
      "Episode  1340 : Reward =  -1280.8018\n",
      "Episode  1341 : Reward =  -2930.9912\n",
      "Episode  1342 : Reward =  -2581.1584\n",
      "Episode  1343 : Reward =  -1341.0914\n",
      "Episode  1344 : Reward =  -1197.744\n",
      "Episode  1345 : Reward =  -1395.9907\n",
      "Episode  1346 : Reward =  -1248.7052\n",
      "Episode  1347 : Reward =  -1058.508\n",
      "Episode  1348 : Reward =  -1384.5378\n",
      "Episode  1349 : Reward =  -2136.5308\n",
      "Episode  1350 : Reward =  -2081.939\n",
      "Episode  1351 : Reward =  -1151.7422\n",
      "Episode  1352 : Reward =  -1130.7485\n",
      "Episode  1353 : Reward =  -1342.1511\n",
      "Episode  1354 : Reward =  -1324.1156\n",
      "Episode  1355 : Reward =  -1702.1139\n",
      "Episode  1356 : Reward =  -1168.778\n",
      "Episode  1357 : Reward =  -2516.2595\n",
      "Episode  1358 : Reward =  -1084.7375\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.16\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1359 : Reward =  -2072.9707\n",
      "Episode  1360 : Reward =  -1074.009\n",
      "Episode  1361 : Reward =  -1134.0917\n",
      "Episode  1362 : Reward =  -3740.4736\n",
      "Episode  1363 : Reward =  -1211.6368\n",
      "Episode  1364 : Reward =  -2880.132\n",
      "Episode  1365 : Reward =  -2036.1545\n",
      "Episode  1366 : Reward =  -1252.5544\n",
      "Episode  1367 : Reward =  -1195.4644\n",
      "Episode  1368 : Reward =  -1262.5914\n",
      "Episode  1369 : Reward =  -1187.3984\n",
      "Episode  1370 : Reward =  -2065.959\n",
      "Episode  1371 : Reward =  -1735.1924\n",
      "Episode  1372 : Reward =  -1175.6484\n",
      "Episode  1373 : Reward =  -2078.493\n",
      "Episode  1374 : Reward =  -1138.2632\n",
      "Episode  1375 : Reward =  -1305.4873\n",
      "Episode  1376 : Reward =  -1159.8927\n",
      "Episode  1377 : Reward =  -1078.7572\n",
      "Episode  1378 : Reward =  -1286.514\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.155\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1379 : Reward =  -3702.3833\n",
      "Episode  1380 : Reward =  -1173.766\n",
      "Episode  1381 : Reward =  -1274.7449\n",
      "Episode  1382 : Reward =  -2162.0137\n",
      "Episode  1383 : Reward =  -1113.6917\n",
      "Episode  1384 : Reward =  -1702.9985\n",
      "Episode  1385 : Reward =  -1312.2032\n",
      "Episode  1386 : Reward =  -1984.3949\n",
      "Episode  1387 : Reward =  -1678.2042\n",
      "Episode  1388 : Reward =  -1132.8403\n",
      "Episode  1389 : Reward =  -1329.156\n",
      "Episode  1390 : Reward =  -997.1667\n",
      "Saving better model at episode 1390 with reward -997.1666870117188\n",
      "Episode  1391 : Reward =  -1146.4448\n",
      "Episode  1392 : Reward =  -1112.2085\n",
      "Episode  1393 : Reward =  -1102.7937\n",
      "Episode  1394 : Reward =  -1079.8658\n",
      "Episode  1395 : Reward =  -1055.3354\n",
      "Episode  1396 : Reward =  -1269.104\n",
      "Episode  1397 : Reward =  -1260.0486\n",
      "Episode  1398 : Reward =  -3683.701\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.15\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1399 : Reward =  -1316.1064\n",
      "Episode  1400 : Reward =  -1161.7874\n",
      "Episode  1401 : Reward =  -1117.4463\n",
      "Episode  1402 : Reward =  -1217.059\n",
      "Episode  1403 : Reward =  -1095.5417\n",
      "Episode  1404 : Reward =  -1678.1975\n",
      "Episode  1405 : Reward =  -1141.4161\n",
      "Episode  1406 : Reward =  -1345.0529\n",
      "Episode  1407 : Reward =  -1167.9142\n",
      "Episode  1408 : Reward =  -1085.2189\n",
      "Episode  1409 : Reward =  -1305.7866\n",
      "Episode  1410 : Reward =  -2158.8242\n",
      "Episode  1411 : Reward =  -1253.0778\n",
      "Episode  1412 : Reward =  -1160.9612\n",
      "Episode  1413 : Reward =  -1690.1035\n",
      "Episode  1414 : Reward =  -2151.3708\n",
      "Episode  1415 : Reward =  -1226.7852\n",
      "Episode  1416 : Reward =  -1242.3561\n",
      "Episode  1417 : Reward =  -2113.1296\n",
      "Episode  1418 : Reward =  -2543.341\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.145\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1419 : Reward =  -1156.0692\n",
      "Episode  1420 : Reward =  -1201.1016\n",
      "Episode  1421 : Reward =  -1107.6669\n",
      "Episode  1422 : Reward =  -1630.8519\n",
      "Episode  1423 : Reward =  -1341.4519\n",
      "Episode  1424 : Reward =  -1058.781\n",
      "Episode  1425 : Reward =  -1236.1765\n",
      "Episode  1426 : Reward =  -1240.3516\n",
      "Episode  1427 : Reward =  -3265.0403\n",
      "Episode  1428 : Reward =  -1125.547\n",
      "Episode  1429 : Reward =  -1652.5686\n",
      "Episode  1430 : Reward =  -1380.3613\n",
      "Episode  1431 : Reward =  -1225.1443\n",
      "Episode  1432 : Reward =  -1190.9264\n",
      "Episode  1433 : Reward =  -1041.8865\n",
      "Episode  1434 : Reward =  -1732.5779\n",
      "Episode  1435 : Reward =  -1022.03235\n",
      "Episode  1436 : Reward =  -1197.2994\n",
      "Episode  1437 : Reward =  -1621.885\n",
      "Episode  1438 : Reward =  -1686.0598\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.14\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1439 : Reward =  -1658.3177\n",
      "Episode  1440 : Reward =  -1249.5771\n",
      "Episode  1441 : Reward =  -1176.2393\n",
      "Episode  1442 : Reward =  -1137.6556\n",
      "Episode  1443 : Reward =  -1118.8125\n",
      "Episode  1444 : Reward =  -2501.1382\n",
      "Episode  1445 : Reward =  -1159.6708\n",
      "Episode  1446 : Reward =  -1231.7499\n",
      "Episode  1447 : Reward =  -1189.4211\n",
      "Episode  1448 : Reward =  -1707.3724\n",
      "Episode  1449 : Reward =  -1159.7467\n",
      "Episode  1450 : Reward =  -3326.6582\n",
      "Episode  1451 : Reward =  -1211.052\n",
      "Episode  1452 : Reward =  -1230.1772\n",
      "Episode  1453 : Reward =  -1683.0406\n",
      "Episode  1454 : Reward =  -1265.7019\n",
      "Episode  1455 : Reward =  -1020.23505\n",
      "Episode  1456 : Reward =  -1120.5469\n",
      "Episode  1457 : Reward =  -2498.8625\n",
      "Episode  1458 : Reward =  -2157.539\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.135\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1459 : Reward =  -2899.8496\n",
      "Episode  1460 : Reward =  -2049.0127\n",
      "Episode  1461 : Reward =  -2072.21\n",
      "Episode  1462 : Reward =  -1168.556\n",
      "Episode  1463 : Reward =  -1349.6274\n",
      "Episode  1464 : Reward =  -1293.4221\n",
      "Episode  1465 : Reward =  -2010.0686\n",
      "Episode  1466 : Reward =  -1248.6124\n",
      "Episode  1467 : Reward =  -1715.9233\n",
      "Episode  1468 : Reward =  -1224.8282\n",
      "Episode  1469 : Reward =  -1261.277\n",
      "Episode  1470 : Reward =  -2479.1372\n",
      "Episode  1471 : Reward =  -1323.7197\n",
      "Episode  1472 : Reward =  -2494.1074\n",
      "Episode  1473 : Reward =  -1772.313\n",
      "Episode  1474 : Reward =  -1246.1073\n",
      "Episode  1475 : Reward =  -1270.5895\n",
      "Episode  1476 : Reward =  -1118.3073\n",
      "Episode  1477 : Reward =  -1185.3192\n",
      "Episode  1478 : Reward =  -2126.576\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.13\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1479 : Reward =  -1730.4277\n",
      "Episode  1480 : Reward =  -1192.0255\n",
      "Episode  1481 : Reward =  -1340.0862\n",
      "Episode  1482 : Reward =  -1203.5663\n",
      "Episode  1483 : Reward =  -1204.8567\n",
      "Episode  1484 : Reward =  -1253.1917\n",
      "Episode  1485 : Reward =  -1276.7401\n",
      "Episode  1486 : Reward =  -1387.3962\n",
      "Episode  1487 : Reward =  -1271.8291\n",
      "Episode  1488 : Reward =  -1446.2603\n",
      "Episode  1489 : Reward =  -1711.2592\n",
      "Episode  1490 : Reward =  -1398.0735\n",
      "Episode  1491 : Reward =  -2910.4316\n",
      "Episode  1492 : Reward =  -1316.672\n",
      "Episode  1493 : Reward =  -1282.2238\n",
      "Episode  1494 : Reward =  -1732.2426\n",
      "Episode  1495 : Reward =  -1118.5754\n",
      "Episode  1496 : Reward =  -1046.8827\n",
      "Episode  1497 : Reward =  -1349.4161\n",
      "Episode  1498 : Reward =  -1801.3838\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.125\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1499 : Reward =  -1752.2915\n",
      "Episode  1500 : Reward =  -1569.6732\n",
      "Episode  1501 : Reward =  -1233.7747\n",
      "Episode  1502 : Reward =  -1343.951\n",
      "Episode  1503 : Reward =  -1698.0605\n",
      "Episode  1504 : Reward =  -1883.4869\n",
      "Episode  1505 : Reward =  -1248.0265\n",
      "Episode  1506 : Reward =  -1201.4536\n",
      "Episode  1507 : Reward =  -1178.6605\n",
      "Episode  1508 : Reward =  -1340.8046\n",
      "Episode  1509 : Reward =  -1235.2675\n",
      "Episode  1510 : Reward =  -1341.2379\n",
      "Episode  1511 : Reward =  -1381.803\n",
      "Episode  1512 : Reward =  -1582.2236\n",
      "Episode  1513 : Reward =  -1706.2483\n",
      "Episode  1514 : Reward =  -1128.6027\n",
      "Episode  1515 : Reward =  -1762.1714\n",
      "Episode  1516 : Reward =  -1176.0266\n",
      "Episode  1517 : Reward =  -1752.2053\n",
      "Episode  1518 : Reward =  -2188.652\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.12\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1519 : Reward =  -1969.3971\n",
      "Episode  1520 : Reward =  -2092.6106\n",
      "Episode  1521 : Reward =  -1143.5719\n",
      "Episode  1522 : Reward =  -1292.0261\n",
      "Episode  1523 : Reward =  -1279.8649\n",
      "Episode  1524 : Reward =  -1275.065\n",
      "Episode  1525 : Reward =  -1142.8529\n",
      "Episode  1526 : Reward =  -1202.5685\n",
      "Episode  1527 : Reward =  -1119.7461\n",
      "Episode  1528 : Reward =  -1590.382\n",
      "Episode  1529 : Reward =  -1224.0886\n",
      "Episode  1530 : Reward =  -2172.1978\n",
      "Episode  1531 : Reward =  -1059.9738\n",
      "Episode  1532 : Reward =  -1658.854\n",
      "Episode  1533 : Reward =  -1197.5206\n",
      "Episode  1534 : Reward =  -1679.4868\n",
      "Episode  1535 : Reward =  -1178.5497\n",
      "Episode  1536 : Reward =  -1288.2126\n",
      "Episode  1537 : Reward =  -1106.2427\n",
      "Episode  1538 : Reward =  -1746.8802\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.115\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1539 : Reward =  -2043.5448\n",
      "Episode  1540 : Reward =  -2424.9905\n",
      "Episode  1541 : Reward =  -1128.9661\n",
      "Episode  1542 : Reward =  -1704.6963\n",
      "Episode  1543 : Reward =  -1185.6554\n",
      "Episode  1544 : Reward =  -1305.0459\n",
      "Episode  1545 : Reward =  -1696.7637\n",
      "Episode  1546 : Reward =  -1159.0958\n",
      "Episode  1547 : Reward =  -1773.6584\n",
      "Episode  1548 : Reward =  -2043.3972\n",
      "Episode  1549 : Reward =  -1048.764\n",
      "Episode  1550 : Reward =  -2436.3003\n",
      "Episode  1551 : Reward =  -1320.3861\n",
      "Episode  1552 : Reward =  -1596.1781\n",
      "Episode  1553 : Reward =  -2370.1226\n",
      "Episode  1554 : Reward =  -1624.2219\n",
      "Episode  1555 : Reward =  -2040.2877\n",
      "Episode  1556 : Reward =  -1700.8131\n",
      "Episode  1557 : Reward =  -2469.7185\n",
      "Episode  1558 : Reward =  -2833.6045\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.11\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1559 : Reward =  -1145.049\n",
      "Episode  1560 : Reward =  -1655.5104\n",
      "Episode  1561 : Reward =  -1155.6646\n",
      "Episode  1562 : Reward =  -2065.0376\n",
      "Episode  1563 : Reward =  -2116.543\n",
      "Episode  1564 : Reward =  -996.53046\n",
      "Saving better model at episode 1564 with reward -996.5304565429688\n",
      "Episode  1565 : Reward =  -1227.4796\n",
      "Episode  1566 : Reward =  -1278.1487\n",
      "Episode  1567 : Reward =  -1700.523\n",
      "Episode  1568 : Reward =  -1250.9402\n",
      "Episode  1569 : Reward =  -1995.0171\n",
      "Episode  1570 : Reward =  -1258.994\n",
      "Episode  1571 : Reward =  -1131.1973\n",
      "Episode  1572 : Reward =  -1202.6293\n",
      "Episode  1573 : Reward =  -1243.4371\n",
      "Episode  1574 : Reward =  -1646.6888\n",
      "Episode  1575 : Reward =  -2033.553\n",
      "Episode  1576 : Reward =  -2054.4053\n",
      "Episode  1577 : Reward =  -1303.6145\n",
      "Episode  1578 : Reward =  -1187.3813\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.105\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1579 : Reward =  -1693.6731\n",
      "Episode  1580 : Reward =  -1048.764\n",
      "Episode  1581 : Reward =  -2075.6592\n",
      "Episode  1582 : Reward =  -1161.8912\n",
      "Episode  1583 : Reward =  -1102.6909\n",
      "Episode  1584 : Reward =  -1588.4419\n",
      "Episode  1585 : Reward =  -2010.979\n",
      "Episode  1586 : Reward =  -1670.522\n",
      "Episode  1587 : Reward =  -1231.8579\n",
      "Episode  1588 : Reward =  -2083.9387\n",
      "Episode  1589 : Reward =  -2786.9075\n",
      "Episode  1590 : Reward =  -1644.6018\n",
      "Episode  1591 : Reward =  -2038.5613\n",
      "Episode  1592 : Reward =  -1310.0066\n",
      "Episode  1593 : Reward =  -1958.8135\n",
      "Episode  1594 : Reward =  -1723.1503\n",
      "Episode  1595 : Reward =  -1138.797\n",
      "Episode  1596 : Reward =  -1250.7347\n",
      "Episode  1597 : Reward =  -2757.3152\n",
      "Episode  1598 : Reward =  -1650.5321\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1599 : Reward =  -1691.4377\n",
      "Episode  1600 : Reward =  -1097.9786\n",
      "Episode  1601 : Reward =  -1277.8198\n",
      "Episode  1602 : Reward =  -1616.0166\n",
      "Episode  1603 : Reward =  -1224.6167\n",
      "Episode  1604 : Reward =  -2200.884\n",
      "Episode  1605 : Reward =  -1655.6001\n",
      "Episode  1606 : Reward =  -1650.7153\n",
      "Episode  1607 : Reward =  -2087.2573\n",
      "Episode  1608 : Reward =  -2522.422\n",
      "Episode  1609 : Reward =  -1294.9319\n",
      "Episode  1610 : Reward =  -1682.592\n",
      "Episode  1611 : Reward =  -1220.5164\n",
      "Episode  1612 : Reward =  -1247.9017\n",
      "Episode  1613 : Reward =  -1204.4855\n",
      "Episode  1614 : Reward =  -1130.3114\n",
      "Episode  1615 : Reward =  -2148.639\n",
      "Episode  1616 : Reward =  -1675.2803\n",
      "Episode  1617 : Reward =  -2083.3145\n",
      "Episode  1618 : Reward =  -1694.2499\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1619 : Reward =  -2493.119\n",
      "Episode  1620 : Reward =  -2100.3335\n",
      "Episode  1621 : Reward =  -2038.1084\n",
      "Episode  1622 : Reward =  -2441.0642\n",
      "Episode  1623 : Reward =  -2431.7725\n",
      "Episode  1624 : Reward =  -2143.0056\n",
      "Episode  1625 : Reward =  -1770.8835\n",
      "Episode  1626 : Reward =  -1168.054\n",
      "Episode  1627 : Reward =  -1258.8767\n",
      "Episode  1628 : Reward =  -2493.15\n",
      "Episode  1629 : Reward =  -1709.1903\n",
      "Episode  1630 : Reward =  -2103.55\n",
      "Episode  1631 : Reward =  -2432.9517\n",
      "Episode  1632 : Reward =  -2729.1\n",
      "Episode  1633 : Reward =  -1161.0092\n",
      "Episode  1634 : Reward =  -1256.0773\n",
      "Episode  1635 : Reward =  -1125.6416\n",
      "Episode  1636 : Reward =  -1241.869\n",
      "Episode  1637 : Reward =  -2515.4016\n",
      "Episode  1638 : Reward =  -2439.488\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1639 : Reward =  -1973.0125\n",
      "Episode  1640 : Reward =  -1990.2666\n",
      "Episode  1641 : Reward =  -2526.306\n",
      "Episode  1642 : Reward =  -2491.3772\n",
      "Episode  1643 : Reward =  -1597.8434\n",
      "Episode  1644 : Reward =  -2407.4534\n",
      "Episode  1645 : Reward =  -1285.6288\n",
      "Episode  1646 : Reward =  -1604.0085\n",
      "Episode  1647 : Reward =  -2879.9587\n",
      "Episode  1648 : Reward =  -3211.751\n",
      "Episode  1649 : Reward =  -2069.5454\n",
      "Episode  1650 : Reward =  -1369.723\n",
      "Episode  1651 : Reward =  -1283.6158\n",
      "Episode  1652 : Reward =  -2350.0293\n",
      "Episode  1653 : Reward =  -1370.5339\n",
      "Episode  1654 : Reward =  -1706.9906\n",
      "Episode  1655 : Reward =  -1202.171\n",
      "Episode  1656 : Reward =  -2890.5427\n",
      "Episode  1657 : Reward =  -2182.381\n",
      "Episode  1658 : Reward =  -2054.9795\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1659 : Reward =  -1260.0479\n",
      "Episode  1660 : Reward =  -2571.5552\n",
      "Episode  1661 : Reward =  -2063.4788\n",
      "Episode  1662 : Reward =  -2162.2478\n",
      "Episode  1663 : Reward =  -2040.4447\n",
      "Episode  1664 : Reward =  -2063.0867\n",
      "Episode  1665 : Reward =  -2153.9358\n",
      "Episode  1666 : Reward =  -4014.4497\n",
      "Episode  1667 : Reward =  -1518.814\n",
      "Episode  1668 : Reward =  -2068.1836\n",
      "Episode  1669 : Reward =  -1714.633\n",
      "Episode  1670 : Reward =  -2554.0862\n",
      "Episode  1671 : Reward =  -2904.441\n",
      "Episode  1672 : Reward =  -1344.2865\n",
      "Episode  1673 : Reward =  -2080.3044\n",
      "Episode  1674 : Reward =  -1096.106\n",
      "Episode  1675 : Reward =  -1323.7594\n",
      "Episode  1676 : Reward =  -1625.0321\n",
      "Episode  1677 : Reward =  -2549.8901\n",
      "Episode  1678 : Reward =  -1723.3892\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1679 : Reward =  -1349.6598\n",
      "Episode  1680 : Reward =  -2070.0117\n",
      "Episode  1681 : Reward =  -2127.5593\n",
      "Episode  1682 : Reward =  -1316.792\n",
      "Episode  1683 : Reward =  -1265.4419\n",
      "Episode  1684 : Reward =  -3327.2466\n",
      "Episode  1685 : Reward =  -2128.627\n",
      "Episode  1686 : Reward =  -1770.9077\n",
      "Episode  1687 : Reward =  -1400.1908\n",
      "Episode  1688 : Reward =  -2432.083\n",
      "Episode  1689 : Reward =  -1231.6671\n",
      "Episode  1690 : Reward =  -1148.0696\n",
      "Episode  1691 : Reward =  -3237.3389\n",
      "Episode  1692 : Reward =  -1206.8164\n",
      "Episode  1693 : Reward =  -2844.42\n",
      "Episode  1694 : Reward =  -1736.8375\n",
      "Episode  1695 : Reward =  -1132.0295\n",
      "Episode  1696 : Reward =  -1665.9084\n",
      "Episode  1697 : Reward =  -2972.4155\n",
      "Episode  1698 : Reward =  -1323.9711\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1699 : Reward =  -2125.7576\n",
      "Episode  1700 : Reward =  -2470.73\n",
      "Episode  1701 : Reward =  -2120.8655\n",
      "Episode  1702 : Reward =  -3325.8582\n",
      "Episode  1703 : Reward =  -2537.6614\n",
      "Episode  1704 : Reward =  -1333.8967\n",
      "Episode  1705 : Reward =  -2860.0085\n",
      "Episode  1706 : Reward =  -1644.4333\n",
      "Episode  1707 : Reward =  -1663.9364\n",
      "Episode  1708 : Reward =  -1698.5132\n",
      "Episode  1709 : Reward =  -2373.1086\n",
      "Episode  1710 : Reward =  -1715.1885\n",
      "Episode  1711 : Reward =  -1246.2043\n",
      "Episode  1712 : Reward =  -1252.0975\n",
      "Episode  1713 : Reward =  -2438.0256\n",
      "Episode  1714 : Reward =  -1679.5458\n",
      "Episode  1715 : Reward =  -2412.683\n",
      "Episode  1716 : Reward =  -1244.6039\n",
      "Episode  1717 : Reward =  -1343.9702\n",
      "Episode  1718 : Reward =  -1326.706\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1719 : Reward =  -1288.0975\n",
      "Episode  1720 : Reward =  -1637.7257\n",
      "Episode  1721 : Reward =  -3642.8755\n",
      "Episode  1722 : Reward =  -1694.6377\n",
      "Episode  1723 : Reward =  -1120.451\n",
      "Episode  1724 : Reward =  -1291.8264\n",
      "Episode  1725 : Reward =  -1901.0957\n",
      "Episode  1726 : Reward =  -1366.5164\n",
      "Episode  1727 : Reward =  -2078.219\n",
      "Episode  1728 : Reward =  -2407.1226\n",
      "Episode  1729 : Reward =  -1628.9727\n",
      "Episode  1730 : Reward =  -2716.6648\n",
      "Episode  1731 : Reward =  -1172.4353\n",
      "Episode  1732 : Reward =  -3306.1528\n",
      "Episode  1733 : Reward =  -1262.0839\n",
      "Episode  1734 : Reward =  -1234.5758\n",
      "Episode  1735 : Reward =  -1638.6439\n",
      "Episode  1736 : Reward =  -3599.934\n",
      "Episode  1737 : Reward =  -2431.3044\n",
      "Episode  1738 : Reward =  -1070.3011\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1739 : Reward =  -2068.7815\n",
      "Episode  1740 : Reward =  -1660.7078\n",
      "Episode  1741 : Reward =  -1636.3895\n",
      "Episode  1742 : Reward =  -1249.4784\n",
      "Episode  1743 : Reward =  -1290.3048\n",
      "Episode  1744 : Reward =  -1667.4695\n",
      "Episode  1745 : Reward =  -1269.2739\n",
      "Episode  1746 : Reward =  -1203.955\n",
      "Episode  1747 : Reward =  -3221.064\n",
      "Episode  1748 : Reward =  -1678.1365\n",
      "Episode  1749 : Reward =  -1648.9138\n",
      "Episode  1750 : Reward =  -1735.8716\n",
      "Episode  1751 : Reward =  -1187.0603\n",
      "Episode  1752 : Reward =  -3286.8533\n",
      "Episode  1753 : Reward =  -2851.0642\n",
      "Episode  1754 : Reward =  -1701.6525\n",
      "Episode  1755 : Reward =  -2494.3967\n",
      "Episode  1756 : Reward =  -1349.154\n",
      "Episode  1757 : Reward =  -2055.0322\n",
      "Episode  1758 : Reward =  -1110.731\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1759 : Reward =  -1959.551\n",
      "Episode  1760 : Reward =  -1286.8025\n",
      "Episode  1761 : Reward =  -2374.1301\n",
      "Episode  1762 : Reward =  -1288.967\n",
      "Episode  1763 : Reward =  -1177.7262\n",
      "Episode  1764 : Reward =  -3219.4302\n",
      "Episode  1765 : Reward =  -1297.475\n",
      "Episode  1766 : Reward =  -1197.9783\n",
      "Episode  1767 : Reward =  -2424.0952\n",
      "Episode  1768 : Reward =  -2751.5757\n",
      "Episode  1769 : Reward =  -1688.5243\n",
      "Episode  1770 : Reward =  -1201.8174\n",
      "Episode  1771 : Reward =  -6235.0723\n",
      "Episode  1772 : Reward =  -2766.693\n",
      "Episode  1773 : Reward =  -1246.4543\n",
      "Episode  1774 : Reward =  -3145.7095\n",
      "Episode  1775 : Reward =  -2064.9246\n",
      "Episode  1776 : Reward =  -3209.544\n",
      "Episode  1777 : Reward =  -2354.1003\n",
      "Episode  1778 : Reward =  -2802.2764\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1779 : Reward =  -2809.4995\n",
      "Episode  1780 : Reward =  -2388.489\n",
      "Episode  1781 : Reward =  -1621.0962\n",
      "Episode  1782 : Reward =  -1603.5397\n",
      "Episode  1783 : Reward =  -2373.0571\n",
      "Episode  1784 : Reward =  -2756.9668\n",
      "Episode  1785 : Reward =  -1964.1552\n",
      "Episode  1786 : Reward =  -2018.6375\n",
      "Episode  1787 : Reward =  -3527.42\n",
      "Episode  1788 : Reward =  -3123.4258\n",
      "Episode  1789 : Reward =  -1095.8958\n",
      "Episode  1790 : Reward =  -1174.1737\n",
      "Episode  1791 : Reward =  -1260.4474\n",
      "Episode  1792 : Reward =  -1306.0488\n",
      "Episode  1793 : Reward =  -1935.7651\n",
      "Episode  1794 : Reward =  -1981.2443\n",
      "Episode  1795 : Reward =  -3439.6667\n",
      "Episode  1796 : Reward =  -1594.7842\n",
      "Episode  1797 : Reward =  -4269.669\n",
      "Episode  1798 : Reward =  -2678.758\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1799 : Reward =  -1984.611\n",
      "Episode  1800 : Reward =  -3934.714\n",
      "Episode  1801 : Reward =  -1958.573\n",
      "Episode  1802 : Reward =  -1976.7596\n",
      "Episode  1803 : Reward =  -1905.401\n",
      "Episode  1804 : Reward =  -3481.6667\n",
      "Episode  1805 : Reward =  -2656.9536\n",
      "Episode  1806 : Reward =  -1988.731\n",
      "Episode  1807 : Reward =  -3164.9336\n",
      "Episode  1808 : Reward =  -1553.6747\n",
      "Episode  1809 : Reward =  -2764.349\n",
      "Episode  1810 : Reward =  -3942.3357\n",
      "Episode  1811 : Reward =  -5438.521\n",
      "Episode  1812 : Reward =  -3510.2046\n",
      "Episode  1813 : Reward =  -1907.7012\n",
      "Episode  1814 : Reward =  -1578.7943\n",
      "Episode  1815 : Reward =  -5439.838\n",
      "Episode  1816 : Reward =  -1984.9365\n",
      "Episode  1817 : Reward =  -1960.7186\n",
      "Episode  1818 : Reward =  -3911.2048\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1819 : Reward =  -3566.512\n",
      "Episode  1820 : Reward =  -1865.3606\n",
      "Episode  1821 : Reward =  -3064.2031\n",
      "Episode  1822 : Reward =  -3110.3608\n",
      "Episode  1823 : Reward =  -4635.154\n",
      "Episode  1824 : Reward =  -3904.2854\n",
      "Episode  1825 : Reward =  -3139.9856\n",
      "Episode  1826 : Reward =  -2305.4722\n",
      "Episode  1827 : Reward =  -2316.5522\n",
      "Episode  1828 : Reward =  -2696.5752\n",
      "Episode  1829 : Reward =  -4235.202\n",
      "Episode  1830 : Reward =  -3444.507\n",
      "Episode  1831 : Reward =  -5007.1606\n",
      "Episode  1832 : Reward =  -4238.6567\n",
      "Episode  1833 : Reward =  -3877.3174\n",
      "Episode  1834 : Reward =  -1140.7206\n",
      "Episode  1835 : Reward =  -4647.0654\n",
      "Episode  1836 : Reward =  -3896.0522\n",
      "Episode  1837 : Reward =  -5403.8047\n",
      "Episode  1838 : Reward =  -3050.062\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1839 : Reward =  -1794.2983\n",
      "Episode  1840 : Reward =  -3857.1902\n",
      "Episode  1841 : Reward =  -1075.3315\n",
      "Episode  1842 : Reward =  -1496.08\n",
      "Episode  1843 : Reward =  -6883.3394\n",
      "Episode  1844 : Reward =  -3006.1233\n",
      "Episode  1845 : Reward =  -2277.787\n",
      "Episode  1846 : Reward =  -2647.7776\n",
      "Episode  1847 : Reward =  -3834.7942\n",
      "Episode  1848 : Reward =  -3488.6333\n",
      "Episode  1849 : Reward =  -3864.4846\n",
      "Episode  1850 : Reward =  -8468.975\n",
      "Episode  1851 : Reward =  -5807.6094\n",
      "Episode  1852 : Reward =  -5023.0215\n",
      "Episode  1853 : Reward =  -3451.9255\n",
      "Episode  1854 : Reward =  -3073.7112\n",
      "Episode  1855 : Reward =  -5694.068\n",
      "Episode  1856 : Reward =  -1383.899\n",
      "Episode  1857 : Reward =  -2695.3423\n",
      "Episode  1858 : Reward =  -3794.8184\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1859 : Reward =  -2670.4614\n",
      "Episode  1860 : Reward =  -3379.8772\n",
      "Episode  1861 : Reward =  -2670.2249\n",
      "Episode  1862 : Reward =  -6972.8013\n",
      "Episode  1863 : Reward =  -2222.1677\n",
      "Episode  1864 : Reward =  -1096.6432\n",
      "Episode  1865 : Reward =  -3808.1462\n",
      "Episode  1866 : Reward =  -3099.4702\n",
      "Episode  1867 : Reward =  -3482.6038\n",
      "Episode  1868 : Reward =  -4581.3135\n",
      "Episode  1869 : Reward =  -2628.6235\n",
      "Episode  1870 : Reward =  -1932.3259\n",
      "Episode  1871 : Reward =  -3048.1484\n",
      "Episode  1872 : Reward =  -1117.4064\n",
      "Episode  1873 : Reward =  -3050.3645\n",
      "Episode  1874 : Reward =  -2329.903\n",
      "Episode  1875 : Reward =  -2357.0854\n",
      "Episode  1876 : Reward =  -3018.054\n",
      "Episode  1877 : Reward =  -1916.8995\n",
      "Episode  1878 : Reward =  -3417.5872\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1879 : Reward =  -3120.9258\n",
      "Episode  1880 : Reward =  -3107.3362\n",
      "Episode  1881 : Reward =  -3034.2295\n",
      "Episode  1882 : Reward =  -2681.5994\n",
      "Episode  1883 : Reward =  -3903.4314\n",
      "Episode  1884 : Reward =  -3456.8928\n",
      "Episode  1885 : Reward =  -4670.019\n",
      "Episode  1886 : Reward =  -6206.88\n",
      "Episode  1887 : Reward =  -1084.2961\n",
      "Episode  1888 : Reward =  -4620.3384\n",
      "Episode  1889 : Reward =  -5025.027\n",
      "Episode  1890 : Reward =  -3082.6575\n",
      "Episode  1891 : Reward =  -2260.1924\n",
      "Episode  1892 : Reward =  -6186.7324\n",
      "Episode  1893 : Reward =  -3426.554\n",
      "Episode  1894 : Reward =  -6174.0317\n",
      "Episode  1895 : Reward =  -3108.7822\n",
      "Episode  1896 : Reward =  -3831.227\n",
      "Episode  1897 : Reward =  -3049.5747\n",
      "Episode  1898 : Reward =  -3800.2603\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1899 : Reward =  -4574.075\n",
      "Episode  1900 : Reward =  -3040.1943\n",
      "Episode  1901 : Reward =  -6543.942\n",
      "Episode  1902 : Reward =  -3466.3228\n",
      "Episode  1903 : Reward =  -3037.341\n",
      "Episode  1904 : Reward =  -5331.7686\n",
      "Episode  1905 : Reward =  -4208.6924\n",
      "Episode  1906 : Reward =  -3411.745\n",
      "Episode  1907 : Reward =  -5748.633\n",
      "Episode  1908 : Reward =  -5369.173\n",
      "Episode  1909 : Reward =  -4569.439\n",
      "Episode  1910 : Reward =  -3785.9846\n",
      "Episode  1911 : Reward =  -1890.0552\n",
      "Episode  1912 : Reward =  -1800.5669\n",
      "Episode  1913 : Reward =  -5711.6313\n",
      "Episode  1914 : Reward =  -2673.4304\n",
      "Episode  1915 : Reward =  -2988.639\n",
      "Episode  1916 : Reward =  -3066.8938\n",
      "Episode  1917 : Reward =  -6533.276\n",
      "Episode  1918 : Reward =  -4186.7476\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1919 : Reward =  -3445.0068\n",
      "Episode  1920 : Reward =  -4122.4956\n",
      "Episode  1921 : Reward =  -3388.1892\n",
      "Episode  1922 : Reward =  -4573.9155\n",
      "Episode  1923 : Reward =  -4570.9697\n",
      "Episode  1924 : Reward =  -2246.8389\n",
      "Episode  1925 : Reward =  -2239.2788\n",
      "Episode  1926 : Reward =  -2625.4963\n",
      "Episode  1927 : Reward =  -3465.231\n",
      "Episode  1928 : Reward =  -1108.8088\n",
      "Episode  1929 : Reward =  -3460.596\n",
      "Episode  1930 : Reward =  -6520.4873\n",
      "Episode  1931 : Reward =  -3819.5715\n",
      "Episode  1932 : Reward =  -2617.3215\n",
      "Episode  1933 : Reward =  -3441.4849\n",
      "Episode  1934 : Reward =  -4159.9297\n",
      "Episode  1935 : Reward =  -4980.835\n",
      "Episode  1936 : Reward =  -4567.9575\n",
      "Episode  1937 : Reward =  -3783.053\n",
      "Episode  1938 : Reward =  -2634.5452\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1939 : Reward =  -4967.1\n",
      "Episode  1940 : Reward =  -2669.4983\n",
      "Episode  1941 : Reward =  -2247.8916\n",
      "Episode  1942 : Reward =  -4573.0845\n",
      "Episode  1943 : Reward =  -3444.2693\n",
      "Episode  1944 : Reward =  -4957.0796\n",
      "Episode  1945 : Reward =  -3825.8523\n",
      "Episode  1946 : Reward =  -2252.7544\n",
      "Episode  1947 : Reward =  -4982.099\n",
      "Episode  1948 : Reward =  -2234.829\n",
      "Episode  1949 : Reward =  -3030.278\n",
      "Episode  1950 : Reward =  -1509.9285\n",
      "Episode  1951 : Reward =  -3888.5825\n",
      "Episode  1952 : Reward =  -1959.355\n",
      "Episode  1953 : Reward =  -3752.623\n",
      "Episode  1954 : Reward =  -3818.7754\n",
      "Episode  1955 : Reward =  -2282.9438\n",
      "Episode  1956 : Reward =  -2307.8967\n",
      "Episode  1957 : Reward =  -2189.5144\n",
      "Episode  1958 : Reward =  -9203.052\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1959 : Reward =  -6159.579\n",
      "Episode  1960 : Reward =  -2687.4602\n",
      "Episode  1961 : Reward =  -3783.863\n",
      "Episode  1962 : Reward =  -2664.0889\n",
      "Episode  1963 : Reward =  -3404.3962\n",
      "Episode  1964 : Reward =  -2647.025\n",
      "Episode  1965 : Reward =  -3796.6892\n",
      "Episode  1966 : Reward =  -3071.243\n",
      "Episode  1967 : Reward =  -1500.8075\n",
      "Episode  1968 : Reward =  -2625.3438\n",
      "Episode  1969 : Reward =  -2681.3958\n",
      "Episode  1970 : Reward =  -2679.355\n",
      "Episode  1971 : Reward =  -2983.9854\n",
      "Episode  1972 : Reward =  -3100.308\n",
      "Episode  1973 : Reward =  -4590.585\n",
      "Episode  1974 : Reward =  -2631.8823\n",
      "Episode  1975 : Reward =  -4954.2417\n",
      "Episode  1976 : Reward =  -4994.1616\n",
      "Episode  1977 : Reward =  -2991.8003\n",
      "Episode  1978 : Reward =  -3453.0286\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1979 : Reward =  -4194.4194\n",
      "Episode  1980 : Reward =  -3061.2693\n",
      "Episode  1981 : Reward =  -1028.6592\n",
      "Episode  1982 : Reward =  -3472.1875\n",
      "Episode  1983 : Reward =  -3777.1184\n",
      "Episode  1984 : Reward =  -2643.9126\n",
      "Episode  1985 : Reward =  -3016.087\n",
      "Episode  1986 : Reward =  -5380.744\n",
      "Episode  1987 : Reward =  -3811.039\n",
      "Episode  1988 : Reward =  -1877.5503\n",
      "Episode  1989 : Reward =  -3438.0461\n",
      "Episode  1990 : Reward =  -2662.664\n",
      "Episode  1991 : Reward =  -6918.0454\n",
      "Episode  1992 : Reward =  -2248.7654\n",
      "Episode  1993 : Reward =  -1853.6195\n",
      "Episode  1994 : Reward =  -1086.5422\n",
      "Episode  1995 : Reward =  -5411.685\n",
      "Episode  1996 : Reward =  -2262.8237\n",
      "Episode  1997 : Reward =  -3029.6277\n",
      "Episode  1998 : Reward =  -1130.867\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode  1999 : Reward =  -3007.7751\n",
      "Episode  2000 : Reward =  -3478.629\n"
     ]
    }
   ],
   "source": [
    "################################### Training ###################################\n",
    "\n",
    "\n",
    "####### initialize environment hyperparameters ######\n",
    "\n",
    "has_continuous_action_space = True #không gian action rời rạc\n",
    "best_reward = -float('inf')  # ban đầu là âm vô cực\n",
    "max_ep_len = 1000                    # max timesteps in one episode\n",
    "max_training_timesteps = int(2000000)   # break training loop if timeteps > max_training_timesteps -- dừng train khi số bước vượt quá\n",
    "\n",
    "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
    "\n",
    "\n",
    "action_std = 0.5\n",
    "action_std_decay_rate = 0.005\n",
    "min_action_std = 0.1\n",
    "action_std_decay_freq = 20000\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "\n",
    "update_timestep = 2048    # update policy every n timesteps -- cập nhật policy\n",
    "K_epochs = 80               # update policy for K epochs -- số lần lặp tối ưu trên 1 batch dữ liệu\n",
    "eps_clip = 0.2              # clip parameter for PPO \n",
    "gamma = 0.99                # discount factor\n",
    "lambda_gae = 0\n",
    "lr_actor = 0.0002       # learning rate for actor network\n",
    "lr_critic = 0.0002       # learning rate for critic network\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "################## Environment parameters ##############\n",
    "lambda_rate = 150\n",
    "D_max = 5\n",
    "xi = 0.01\n",
    "max_power = - (2 ** (lambda_rate/200)-1)/(math.log10(1-(xi**(D_max**-1)/D_max)))\n",
    "snr_feedback=True,# Có phản hồi SNR\n",
    "harq_type='IR'\n",
    "\n",
    "env = ENV_paper(lambda_rate, D_max, xi, max_power, snr_feedback, harq_type)\n",
    "\n",
    "# state space dimension\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# action space dimension\n",
    "if has_continuous_action_space:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "else:\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "# print(\"save checkpoint path : \" + checkpoint_path)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "############# print all hyperparameters #############\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"max training timesteps : \", max_training_timesteps)\n",
    "print(\"max timesteps per episode : \", max_ep_len)\n",
    "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
    "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"state space dimension : \", state_dim)\n",
    "print(\"action space dimension : \", action_dim)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "if has_continuous_action_space:\n",
    "    print(\"Initializing a continuous action space policy\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"starting std of action distribution : \", action_std)\n",
    "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
    "    print(\"minimum std of action distribution : \", min_action_std)\n",
    "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
    "\n",
    "else:\n",
    "    print(\"Initializing a discrete action space policy\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\")\n",
    "print(\"PPO K epochs : \", K_epochs)\n",
    "print(\"PPO epsilon clip : \", eps_clip)\n",
    "print(\"discount factor (gamma) : \", gamma)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"optimizer learning rate actor : \", lr_actor)\n",
    "print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "################# training procedure ################\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma,lambda_gae, K_epochs, eps_clip, has_continuous_action_space, action_std, minibatch_size = 128)\n",
    "\n",
    "\n",
    "# printing and logging variables\n",
    "# print_running_reward = 0\n",
    "# print_running_episodes = 0\n",
    "\n",
    "# log_running_reward = 0\n",
    "# log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "\n",
    "# training loop\n",
    "while time_step <= max_training_timesteps: #vòng lặp dừng khi vượt quá max_time_step\n",
    "\n",
    "    state = env.reset() #trả về trạng thái ban đầu \n",
    "\n",
    "    # Check if state is tuple\n",
    "    if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "\n",
    "    current_ep_reward = 0 #tổng phần thưởng trong episode hiện tại\n",
    "\n",
    "    for t in range(1, max_ep_len+1): #chạy tối đa 500 bước\n",
    "\n",
    "        # select action with policy\n",
    "        action = ppo_agent.select_action(state) #chọn action cho state dự vào policy hiện tại\n",
    "        state, reward, done, info = env.step(action) #thực hiện hành động và nhận phản hồi\n",
    "\n",
    "        # saving reward and is_terminals vào buffer\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done \n",
    "        )\n",
    "\n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "        #print(info)\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update() #cập nhật lại policy nếu đủ số bước\n",
    "\n",
    "        # if continuous action space; then decay action std of ouput action distribution\n",
    "        # Nếu không gian hành động là liên tục, giảm độ lệch chuẩn để chuyển từ khám phá sang khai thác\n",
    "\n",
    "\n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "    if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "        ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "    print('Episode ', i_episode, ': Reward = ', current_ep_reward)\n",
    "\n",
    "    # print_running_reward += current_ep_reward\n",
    "    # print_running_episodes += 1\n",
    "\n",
    "    # log_running_reward += current_ep_reward\n",
    "    # log_running_episodes += 1\n",
    "    # Save model if reward improves\n",
    "    \n",
    "    if current_ep_reward >= best_reward:\n",
    "        print(f\"Saving better model at episode {i_episode} with reward {current_ep_reward}\")\n",
    "        best_reward = current_ep_reward\n",
    "        torch.save(ppo_agent.policy.state_dict(), 'ppo_best_model_paper.pth')\n",
    "\n",
    "    i_episode += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b2b61858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(ppo_agent, model_path, env, num_episodes=50, max_ep_len=1000):\n",
    "    \"\"\"\n",
    "    Function to evaluate the PPO model over multiple episodes and calculate average results.\n",
    "    \n",
    "    Args:\n",
    "        ppo_agent (PPO): The initialized PPO agent.\n",
    "        model_path (str): Path to the saved model file.\n",
    "        env (gym.Env): The environment to evaluate.\n",
    "        num_episodes (int): Number of episodes to evaluate (default is 50).\n",
    "        max_ep_len (int): Maximum number of steps per episode (default is 1000).\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the saved model state into the policy\n",
    "    ppo_agent.policy.load_state_dict(torch.load(model_path))\n",
    "    ppo_agent.policy.eval()\n",
    "\n",
    "    total_powers = []\n",
    "    total_rewards = []\n",
    "    total_delay_violations = []\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "\n",
    "        episode_power = 0.0\n",
    "        episode_reward = 0.0\n",
    "        delay_violation = 0\n",
    "\n",
    "        for t in range(1, max_ep_len + 1):\n",
    "            # Convert state to tensor\n",
    "            state_tensor = torch.FloatTensor(state).to(device)\n",
    "            \n",
    "            # Get the mean action from the actor\n",
    "            with torch.no_grad():\n",
    "                action_mean = ppo_agent.policy.actor(state_tensor)\n",
    "            \n",
    "            # Convert to numpy array for the environment\n",
    "            action = action_mean.cpu().numpy()\n",
    "            \n",
    "            # Perform the action in the environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            state = next_state\n",
    "\n",
    "            # Get information from info\n",
    "            power = info.get('power', 0.0)\n",
    "            delay_violation = info.get('delay_violation', 0)\n",
    "            \n",
    "            # Accumulate power and reward\n",
    "            episode_power += power\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Print information for each step\n",
    "            #print(f\"Episode {episode}, Step {t}: Power = {power:.2f}, Delay Violations (dt) = {delay_violation}, Reward = {reward:.2f}\")\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Calculate average power for the episode\n",
    "        average_power = episode_power / t if t > 0 else 0.0\n",
    "\n",
    "        # Store results of the episode\n",
    "        total_powers.append(episode_power)\n",
    "        total_rewards.append(episode_reward)\n",
    "        total_delay_violations.append(delay_violation)\n",
    "\n",
    "        # # Print summary for the episode\n",
    "        # print(f\"\\n--- Summary of Episode {episode} ---\")\n",
    "        # print(f\"Total transmitted power: {episode_power:.2f}\")\n",
    "        # print(f\"Average power: {average_power:.2f}\")\n",
    "        # print(f\"Total delay violations: {delay_violation}\")\n",
    "        # print(f\"Total Reward: {episode_reward:.2f}\\n\")\n",
    "\n",
    "    # Calculate average results after 50 episodes\n",
    "    avg_total_power = np.mean(total_powers)\n",
    "    avg_average_power = np.mean([power / max_ep_len for power in total_powers])\n",
    "    avg_total_reward = np.mean(total_rewards)\n",
    "    avg_delay_violations = np.mean(total_delay_violations)\n",
    "\n",
    "    print(\"\\n--- Average results after 50 episodes ---\")\n",
    "    print(f\"Average total transmitted power: {avg_total_power:.2f}\")\n",
    "    print(f\"Average power per episode: {avg_average_power:.2f}\")\n",
    "    print(f\"Average total Reward: {avg_total_reward:.2f}\")\n",
    "    print(f\"Average number of delay violations: {avg_delay_violations:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "42e5866e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Average results after 50 episodes ---\n",
      "Average total transmitted power: 1195.15\n",
      "Average power per episode: 1.20\n",
      "Average total Reward: -1614.57\n",
      "Average number of delay violations: 9.66\n"
     ]
    }
   ],
   "source": [
    "model_path = 'ppo_best_model_paper.pth'\n",
    "evaluate_model(ppo_agent, model_path, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
